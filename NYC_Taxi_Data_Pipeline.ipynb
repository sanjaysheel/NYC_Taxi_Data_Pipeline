{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Flow Diagram\n",
    "\n",
    "\n",
    "Here’s a textual flow diagram for a simplified **NYC Taxi Data ETL Workflow**. You can easily adapt it into a tool like Lucidchart or draw.io for visual representation.\n",
    "\n",
    "---\n",
    "\n",
    "### **Simplified Data Flow Diagram**\n",
    "\n",
    "1. **Data Sources**:\n",
    "   - NYC Taxi Trip Data  \n",
    "   - Driver Metadata  \n",
    "   - Weather Data  \n",
    "\n",
    "   **(Data Flow →)**\n",
    "\n",
    "2. **Data Ingestion**:\n",
    "   - All data ingested into **Amazon S3** under the `/raw/` folder.  \n",
    "\n",
    "   **(Data Flow →)**\n",
    "\n",
    "3. **Schema Inference**:\n",
    "   - **AWS Glue Crawler** scans the raw data in S3.  \n",
    "   - Schema information is stored in the **AWS Glue Data Catalog**.  \n",
    "\n",
    "   **(Data Flow →)**\n",
    "\n",
    "4. **ETL Process**:\n",
    "   - **AWS Glue Job** performs:\n",
    "     - Data Cleaning: Remove invalid data.  \n",
    "     - Data Enrichment: Combine with metadata (drivers, vehicles, and weather).  \n",
    "     - Data Partitioning: Store processed data partitioned by year/month in **Amazon S3** `/processed/` in **Parquet** format.  \n",
    "\n",
    "   **(Data Flow →)**\n",
    "\n",
    "5. **Query Layer**:\n",
    "   - **Amazon Athena** queries the processed data stored in S3 using the schema from Glue Data Catalog.  \n",
    "\n",
    "   **(Data Flow →)**\n",
    "\n",
    "6. **Visualization**:\n",
    "   - **Amazon QuickSight** creates dashboards for:\n",
    "     - Driver performance.  \n",
    "     - Revenue trends.  \n",
    "     - Weather impact on trips.  \n",
    "\n",
    "---\n",
    "\n",
    "This flow keeps things simple while maintaining clarity. You can create a visual flowchart by placing these components in rectangles and connecting them with arrows labeled \"Data Flow.\" Let me know if you'd like additional details!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let me simplify the architecture diagram and make it more intuitive and easier to understand. I'll focus on fewer components while maintaining clarity around the workflow. Here's the simplified plan:\n",
    "\n",
    "---\n",
    "\n",
    "### **Simplified Architecture Plan**\n",
    "\n",
    "1. **Data Sources**:\n",
    "   - NYC Taxi Trip Data, Driver Metadata, and Weather Data.\n",
    "\n",
    "2. **Ingestion and Storage**:\n",
    "   - All data is ingested into **Amazon S3** under the `/raw/` folder.\n",
    "\n",
    "3. **Schema Inference**:\n",
    "   - **AWS Glue Crawler** scans the raw data and populates the **AWS Glue Data Catalog**.\n",
    "\n",
    "4. **ETL Process**:\n",
    "   - A single **AWS Glue Job** performs:\n",
    "     - Data Cleaning.\n",
    "     - Enrichment (joins with metadata and weather data).\n",
    "     - Transformations (e.g., aggregations, derived features).\n",
    "     - Outputs **Parquet** files in **S3** under `/processed/`, partitioned by year/month.\n",
    "\n",
    "5. **Query and Analyze**:\n",
    "   - **Amazon Athena** queries the processed data directly from S3.\n",
    "\n",
    "6. **Visualization**:\n",
    "   - **Amazon QuickSight** creates dashboards for insights.\n",
    "\n",
    "---\n",
    "\n",
    "Would you like me to generate a visual diagram based on this simpler workflow? Let me know if there are any specific areas you'd like to keep or remove!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. **Data Ingestion and Storage**\n",
    "2. **Data Transformation and Analysis**\n",
    "   - SQL Functions\n",
    "   - Window Functions\n",
    "   - Slowly Changing Dimensions (SCD)\n",
    "   - Lead and Lag Functions\n",
    "3. **Interview Topics Coverage**\n",
    "4. **Performance Optimization**\n",
    "   - Memory Management\n",
    "   - Time Management\n",
    "   - Other Techniques"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Yes, the project covers all these aspects comprehensively. Here's how each point is addressed:\n",
    "\n",
    "---\n",
    "\n",
    "### **1. Data Ingestion and Storage**\n",
    "   - **Data Sources**: NYC Taxi trip data, driver/vehicle metadata, and weather data.\n",
    "   - **Ingestion**:\n",
    "     - Raw data is stored in **Amazon S3** under `raw/`.\n",
    "     - Schema is inferred using **Glue Crawlers**.\n",
    "   - **Storage**:\n",
    "     - Cleaned, enriched, and aggregated data is stored in S3 under `processed/` in Parquet format.\n",
    "     - Partitioning is applied (e.g., by year/month) for optimized querying.\n",
    "\n",
    "---\n",
    "\n",
    "### **2. Data Transformation and Analysis**\n",
    "\n",
    "#### **a. SQL Functions**\n",
    "   - **Examples** in Athena Queries:\n",
    "     - Aggregations (e.g., `SUM`, `AVG`, `COUNT`).\n",
    "     - Filtering and grouping data (`GROUP BY`, `HAVING`).\n",
    "     - Time-based SQL functions (`EXTRACT`, `DAYOFWEEK`).\n",
    "   - Examples:\n",
    "     ```sql\n",
    "     SELECT EXTRACT(HOUR FROM pickup_time) AS hour, SUM(fare_amount) AS total_revenue\n",
    "     FROM enriched_trips\n",
    "     GROUP BY hour;\n",
    "     ```\n",
    "\n",
    "#### **b. Window Functions**\n",
    "   - **Rank Drivers by Revenue**:\n",
    "     ```python\n",
    "     window_spec = Window.partitionBy(\"driver_id\").orderBy(F.desc(\"fare_amount\"))\n",
    "     df = df.withColumn(\"rank\", F.rank().over(window_spec))\n",
    "     ```\n",
    "   - Cumulative Revenue:\n",
    "     ```python\n",
    "     df = df.withColumn(\"cumulative_revenue\", F.sum(\"fare_amount\").over(window_spec))\n",
    "     ```\n",
    "\n",
    "#### **c. Slowly Changing Dimensions (SCD)**\n",
    "   - Implemented **SCD Type 2** to maintain historical changes in driver and vehicle metadata.\n",
    "   - Tracks `start_date`, `end_date`, and `is_current` flags for records.\n",
    "\n",
    "#### **d. Lead and Lag Functions**\n",
    "   - Applied to compare trips (e.g., fare trends between consecutive rides).\n",
    "   ```python\n",
    "   df = df.withColumn(\"lead_fare\", F.lead(\"fare_amount\").over(window_spec))\n",
    "   df = df.withColumn(\"lag_fare\", F.lag(\"fare_amount\").over(window_spec))\n",
    "   ```\n",
    "\n",
    "---\n",
    "\n",
    "### **3. Interview Topics Coverage**\n",
    "\n",
    "#### **Covered Topics**:\n",
    "1. **Data Processing**:\n",
    "   - Raw data cleaning.\n",
    "   - Advanced feature engineering.\n",
    "   - SQL transformations.\n",
    "2. **Glue-Specific**:\n",
    "   - Glue Crawlers for schema inference.\n",
    "   - Glue Jobs with PySpark for ETL.\n",
    "   - Partitioning and Parquet optimizations.\n",
    "3. **Window Functions**:\n",
    "   - Ranking, lead/lag, cumulative aggregates.\n",
    "4. **SCD Type 2**:\n",
    "   - Historical tracking with start/end dates.\n",
    "5. **Performance Optimization**:\n",
    "   - Partitioning and bucketing for faster queries in Athena.\n",
    "   - Minimizing I/O using columnar storage (Parquet).\n",
    "   - Optimizing Glue job configurations.\n",
    "6. **Athena SQL Queries**:\n",
    "   - KPIs like revenue per driver, busiest pickup locations, and weather impact.\n",
    "7. **Visualization**:\n",
    "   - Dashboards in Amazon QuickSight.\n",
    "\n",
    "---\n",
    "\n",
    "### **4. Performance Optimization**\n",
    "\n",
    "#### **a. Memory Management**\n",
    "   - **Glue Job Optimizations**:\n",
    "     - Use `DynamicFrame.toDF()` only when necessary to minimize memory usage.\n",
    "     - Persist intermediate DataFrames using `cache()` to avoid recomputation.\n",
    "   - **Partitioning**:\n",
    "     - Data partitioned by `year` and `month` in S3 for reduced data scans in Athena.\n",
    "   - **File Formats**:\n",
    "     - Parquet used for columnar storage, reducing file size and improving query performance.\n",
    "\n",
    "#### **b. Time Management**\n",
    "   - **ETL Job Tuning**:\n",
    "     - Spark configurations adjusted for efficient execution:\n",
    "       - `spark.sql.shuffle.partitions` reduced for smaller datasets.\n",
    "       - `spark.executor.memory` and `spark.executor.cores` tuned for job scale.\n",
    "   - **Athena Query Optimization**:\n",
    "     - Minimized query time by creating partitioned tables.\n",
    "     - Avoided unnecessary large table scans with filters (`WHERE` clauses).\n",
    "\n",
    "#### **c. Other Techniques**\n",
    "   - **Join Optimization**:\n",
    "     - Broadcast joins for small datasets like weather and metadata.\n",
    "   - **Incremental Data Processing**:\n",
    "     - Process only new/changed data using Glue Job bookmarks.\n",
    "   - **Data Sampling for Development**:\n",
    "     - Work with smaller subsets of data locally in Spark to speed up testing.\n",
    "\n",
    "---\n",
    "\n",
    "Let me know if you'd like enhancements or more specific code examples for any of these areas!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here’s an updated version of the **large-scale ETL project** without **Redshift**. We'll rely solely on **S3, Glue, Athena, and QuickSight** for data processing, querying, and visualization.\n",
    "\n",
    "---\n",
    "\n",
    "## **Updated Project Architecture**\n",
    "\n",
    "### **Components Overview**\n",
    "1. **Data Sources**:\n",
    "   - NYC Taxi data (multi-year datasets).\n",
    "   - Driver and vehicle metadata.\n",
    "   - Weather data for correlation with trip performance.\n",
    "2. **AWS Glue**:\n",
    "   - Glue Crawlers to infer schemas.\n",
    "   - Glue ETL Jobs for advanced transformations.\n",
    "3. **AWS Athena**:\n",
    "   - Query transformed and aggregated data directly from S3.\n",
    "4. **Amazon QuickSight**:\n",
    "   - Dashboards for analytics and reporting.\n",
    "\n",
    "---\n",
    "\n",
    "## **Step-by-Step Implementation**\n",
    "\n",
    "### **1. Prepare Data**\n",
    "Upload datasets to S3 with the following structure:\n",
    "- NYC Taxi data: `s3://nyc-taxi-data-bucket/raw/trips/`\n",
    "- Driver metadata: `s3://nyc-taxi-data-bucket/raw/driver_metadata/`\n",
    "- Vehicle metadata: `s3://nyc-taxi-data-bucket/raw/vehicle_metadata/`\n",
    "- Weather data: `s3://nyc-taxi-data-bucket/raw/weather_data/`\n",
    "\n",
    "---\n",
    "\n",
    "### **2. Glue Crawler Setup**\n",
    "- **Crawlers**:\n",
    "  - NYC Taxi dataset (`raw/trips/`).\n",
    "  - Driver and vehicle metadata (`raw/driver_metadata/` and `raw/vehicle_metadata/`).\n",
    "  - Weather data (`raw/weather_data/`).\n",
    "\n",
    "- **Output**:\n",
    "  - Glue Data Catalog tables:\n",
    "    - `nyc_taxi_raw_trips`\n",
    "    - `driver_metadata`\n",
    "    - `vehicle_metadata`\n",
    "    - `weather_data`\n",
    "\n",
    "---\n",
    "\n",
    "### **3. Glue Jobs for Complex ETL**\n",
    "\n",
    "#### **Job 1: Data Cleaning and Standardization**\n",
    "1. **Input**: NYC Taxi raw trip data.\n",
    "2. **Transformations**:\n",
    "   - Remove records with invalid or missing values.\n",
    "   - Standardize column names.\n",
    "   - Parse and validate timestamps.\n",
    "   - Drop unnecessary columns.\n",
    "\n",
    "```python\n",
    "df = df.filter((df[\"fare_amount\"] > 0) & (df[\"passenger_count\"] > 0))\n",
    "df = df.withColumnRenamed(\"pickup_datetime\", \"pickup_time\").withColumnRenamed(\"dropoff_datetime\", \"dropoff_time\")\n",
    "df = df.withColumn(\"pickup_time\", F.to_timestamp(\"pickup_time\", \"yyyy-MM-dd HH:mm:ss\"))\n",
    "df = df.withColumn(\"dropoff_time\", F.to_timestamp(\"dropoff_time\", \"yyyy-MM-dd HH:mm:ss\"))\n",
    "```\n",
    "\n",
    "3. **Output**:\n",
    "   - Save as Parquet to `s3://nyc-taxi-data-bucket/processed/cleaned_trips/`.\n",
    "\n",
    "---\n",
    "\n",
    "#### **Job 2: Advanced Transformations**\n",
    "1. **Input**: Cleaned trip data, weather data, and metadata.\n",
    "2. **Transformations**:\n",
    "   - **Join Datasets**:\n",
    "     - Enrich trip data with weather, driver, and vehicle metadata.\n",
    "   - **Derived Features**:\n",
    "     - Time-based features: Hour of day, day of week, and trip duration.\n",
    "     - Revenue metrics: Revenue per mile, revenue per minute.\n",
    "     - Categorize trips by distance (short/medium/long).\n",
    "   - **Window Functions**:\n",
    "     - Rank drivers by daily or monthly revenue.\n",
    "     - Compute cumulative revenue for each driver.\n",
    "\n",
    "```python\n",
    "# Join with Weather Data\n",
    "df = df.join(weather_df, \"date\", \"left\")\n",
    "\n",
    "# Add Time Features\n",
    "df = df.withColumn(\"hour_of_day\", F.hour(\"pickup_time\"))\n",
    "df = df.withColumn(\"trip_duration\", (F.col(\"dropoff_time\").cast(\"long\") - F.col(\"pickup_time\").cast(\"long\")) / 60)\n",
    "\n",
    "# Revenue Metrics\n",
    "df = df.withColumn(\"revenue_per_mile\", F.col(\"fare_amount\") / F.col(\"trip_distance\"))\n",
    "df = df.withColumn(\"revenue_per_minute\", F.col(\"fare_amount\") / F.col(\"trip_duration\"))\n",
    "\n",
    "# Driver Ranking\n",
    "window_spec = Window.partitionBy(\"driver_id\").orderBy(F.desc(\"fare_amount\"))\n",
    "df = df.withColumn(\"driver_rank\", F.rank().over(window_spec))\n",
    "```\n",
    "\n",
    "3. **Output**:\n",
    "   - Save as partitioned Parquet by year/month: `s3://nyc-taxi-data-bucket/processed/enriched_trips/`.\n",
    "\n",
    "---\n",
    "\n",
    "#### **Job 3: Aggregations and SCD**\n",
    "1. **Input**: Enriched trip data and metadata.\n",
    "2. **Transformations**:\n",
    "   - **SCD Type 2**:\n",
    "     - Track changes in driver or vehicle metadata over time.\n",
    "   - **Aggregations**:\n",
    "     - Monthly and daily revenue by:\n",
    "       - Driver.\n",
    "       - Vehicle type.\n",
    "       - Pickup location.\n",
    "   - **Advanced Grouping**:\n",
    "     - Compute peak vs. non-peak revenue distributions.\n",
    "\n",
    "```python\n",
    "# Example Aggregation: Monthly Revenue\n",
    "monthly_revenue = df.groupBy(\"driver_id\", \"month\").agg(\n",
    "    F.sum(\"fare_amount\").alias(\"total_revenue\"),\n",
    "    F.avg(\"trip_distance\").alias(\"avg_distance\")\n",
    ")\n",
    "\n",
    "# SCD Type 2 Example\n",
    "driver_history = driver_df.withColumn(\"start_date\", current_date()).withColumn(\"is_current\", lit(True))\n",
    "```\n",
    "\n",
    "3. **Output**:\n",
    "   - Aggregated data saved to `s3://nyc-taxi-data-bucket/processed/aggregates/`.\n",
    "\n",
    "---\n",
    "\n",
    "### **4. Query Processed Data in Athena**\n",
    "\n",
    "1. **Create Athena Tables**:\n",
    "   - Use the Glue Data Catalog for processed data (`processed/cleaned_trips/`, `processed/enriched_trips/`, `processed/aggregates/`).\n",
    "\n",
    "2. **Example Queries**:\n",
    "   - **Top 10 Drivers by Revenue**:\n",
    "     ```sql\n",
    "     SELECT driver_id, SUM(total_revenue) AS total_revenue\n",
    "     FROM aggregates\n",
    "     GROUP BY driver_id\n",
    "     ORDER BY total_revenue DESC\n",
    "     LIMIT 10;\n",
    "     ```\n",
    "   - **Revenue vs. Weather Analysis**:\n",
    "     ```sql\n",
    "     SELECT weather_condition, SUM(fare_amount) AS total_revenue\n",
    "     FROM enriched_trips\n",
    "     GROUP BY weather_condition\n",
    "     ORDER BY total_revenue DESC;\n",
    "     ```\n",
    "   - **Peak vs Non-Peak Revenue**:\n",
    "     ```sql\n",
    "     SELECT hour_of_day, SUM(fare_amount) AS total_revenue\n",
    "     FROM enriched_trips\n",
    "     GROUP BY hour_of_day\n",
    "     ORDER BY hour_of_day;\n",
    "     ```\n",
    "\n",
    "---\n",
    "\n",
    "### **5. Visualization with QuickSight**\n",
    "- Create dashboards in QuickSight for:\n",
    "  - **Driver Performance**: Revenue trends, top 10 drivers, and trip counts.\n",
    "  - **Weather Impact**: Revenue distributions by weather conditions.\n",
    "  - **Peak/Non-Peak Hours**: Hourly revenue trends.\n",
    "\n",
    "---\n",
    "\n",
    "### **6. Local Spark Setup**\n",
    "- Use PySpark locally to validate Glue job transformations.\n",
    "- Example setup:\n",
    "  - Use the same Parquet files from S3 for testing.\n",
    "  - Libraries: `boto3` for S3, `pyspark`.\n",
    "\n",
    "---\n",
    "\n",
    "## **Additional Transformations**\n",
    "1. **Geospatial Analytics**:\n",
    "   - Identify busiest pickup/drop-off zones using latitude/longitude clustering (e.g., KMeans).\n",
    "2. **Anomaly Detection**:\n",
    "   - Flag unusually high or low fares using z-scores.\n",
    "3. **Trip Time Optimization**:\n",
    "   - Analyze average speeds by hour or weather conditions.\n",
    "\n",
    "---\n",
    "\n",
    "### **Interview Preparation Questions**\n",
    "1. **Explain SCD Type 2 in AWS Glue.**\n",
    "2. **How do you optimize Glue jobs for large-scale transformations?**\n",
    "3. **What are the benefits of partitioning data in S3?**\n",
    "4. **Describe a use case for window functions in Spark.**\n",
    "5. **How does Athena handle partitioned data stored in S3?**\n",
    "\n",
    "Let me know if you'd like detailed implementation scripts or help deploying this!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
