{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you want to practice and understand Multipart Upload using your 150MB Parquet file, you can still split and upload it in multiple parts, even though it's not required for such a small file. Below is a step-by-step guide to do that using Boto3."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import boto3\n",
    "import os\n",
    "\n",
    "s3 = boto3.client('s3')\n",
    "# bucket_name = 'my-bucket-name'\n",
    "# file_path = 'myfile.parquet'\n",
    "# s3_key = 'uploaded/myfile.parquet'  # Path in S3\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'nyc-taxi-data-pipeline'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "s3.list_buckets()[\"Buckets\"][0]['Name']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = 's3://nyc-taxi-data-pipeline/multipart-parquet/'\n",
    "bucket_name = 'nyc-taxi-data-pipeline'\n",
    "file_path = '/mount_folder/alpha/multipart_s3_upload/part-00000-2224c996-15d6-400a-8ae4-2d0740e74c18.c000.gz.parquet'\n",
    "s3_key = 'multipart-parquet/Multipart.parquet'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Upload ID: nX8s_GJnmoxp_DkU.Xl0V1S4PLXKjjo_iqePeo81VQOvNJAqNZ129uwJ63C8MPdpaaPyPzRvZlS5hbdWVLo.AxRZYbaMTu7aLGzlYxAvZOFRTCdPHX0GHTnyn6g2UDNE\n"
     ]
    }
   ],
   "source": [
    "response = s3.create_multipart_upload(Bucket=bucket_name, Key=s3_key)\n",
    "upload_id = response['UploadId']\n",
    "print(f\"Upload ID: {upload_id}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Uploaded Part 1\n",
      "Uploaded Part 2\n",
      "Uploaded Part 3\n"
     ]
    }
   ],
   "source": [
    "chunk_size = 50 * 1024 * 1024  # 50MB per part\n",
    "parts = []\n",
    "\n",
    "with open(file_path, 'rb') as f:\n",
    "    part_number = 1\n",
    "    while True:\n",
    "        data = f.read(chunk_size)\n",
    "        if not data:\n",
    "            break\n",
    "        response = s3.upload_part(\n",
    "            Bucket=bucket_name,\n",
    "            Key=s3_key,\n",
    "            PartNumber=part_number,\n",
    "            UploadId=upload_id,\n",
    "            Body=data\n",
    "        )\n",
    "        parts.append({'ETag': response['ETag'], 'PartNumber': part_number})\n",
    "        print(f\"Uploaded Part {part_number}\")\n",
    "        part_number += 1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'ETag': '\"048b48651b16bfc00b73d4c3d9638f29\"', 'PartNumber': 1},\n",
       " {'ETag': '\"771e16630fbed987bd913ea456b714b0\"', 'PartNumber': 2},\n",
       " {'ETag': '\"b4c187729ee3feeeeb9277c79e2be891\"', 'PartNumber': 3}]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "parts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Multipart Upload Completed!\n"
     ]
    }
   ],
   "source": [
    "s3.complete_multipart_upload(\n",
    "    Bucket=bucket_name,\n",
    "    Key=s3_key,\n",
    "    UploadId=upload_id,\n",
    "    MultipartUpload={'Parts': parts}\n",
    ")\n",
    "print(\"Multipart Upload Completed!\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "No, the uploaded file in **S3** will be the **same as the original file** after **completing the multipart upload**. S3 automatically **reassembles** the parts into a single object once the `complete_multipart_upload` API is called.\n",
    "\n",
    "However, if you want to **verify** the upload:\n",
    "- **Download the file from S3** and compare it with the original.\n",
    "- **Check the file size and content** after downloading.\n",
    "\n",
    "### **Verify by Downloading the File**\n",
    "```python\n",
    "s3.download_file(bucket_name, s3_key, 'downloaded_myfile.parquet')\n",
    "\n",
    "# Check file size\n",
    "import os\n",
    "print(\"Original file size:\", os.path.getsize(file_path))\n",
    "print(\"Downloaded file size:\", os.path.getsize('downloaded_myfile.parquet'))\n",
    "```\n",
    "\n",
    "If the sizes match, the upload was **successful**, and the file is the same as the original.\n",
    "\n",
    "Would you like to see how to **split the file locally** before uploading? That way, you can inspect the parts before they go to S3. Let me know how deep you want to go! ðŸš€"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original file size: 139528869\n",
      "Downloaded file size: 139528869\n"
     ]
    }
   ],
   "source": [
    "s3.download_file(bucket_name, s3_key, 'Multipart.parquet')\n",
    "\n",
    "# Check file size\n",
    "import os\n",
    "print(\"Original file size:\", os.path.getsize(file_path))\n",
    "print(\"Downloaded file size:\", os.path.getsize('Multipart.parquet'))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Check while merging 2 files into single and upload as multipart "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Merged file size: 277963098 bytes\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "# Define input and output files\n",
    "file1 = '/mount_folder/alpha/multipart_s3_upload/part-00000-2224c996-15d6-400a-8ae4-2d0740e74c18.c000.gz.parquet'\n",
    "file2 = '/mount_folder/alpha/multipart_s3_upload/part-00002-6409130e-6e9b-44d1-90ec-9f9d0ade2504.c000.gz.parquet'\n",
    "merged_file = 'merged_file.parquet'\n",
    "\n",
    "# Merge files\n",
    "with open(merged_file, 'wb') as outfile:\n",
    "    for fname in [file1, file2]:\n",
    "        with open(fname, 'rb') as infile:\n",
    "            outfile.write(infile.read())\n",
    "\n",
    "# Check file size after merging\n",
    "print(f\"Merged file size: {os.path.getsize(merged_file)} bytes\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Upload ID: QvG.mpVegbgmA3a.Owbz8d3PrD5.F3tAAhzV2y6r3MgQNu6pLrRo1irs3Sj74TzT2ARP.IpAfWu1NwBshivbed9VdOe.TnE_S_wrZy0EoLwgWMG52awPEheUQfUSiLdI\n",
      "Uploaded Part 1\n",
      "Uploaded Part 2\n",
      "Uploaded Part 3\n",
      "Uploaded Part 4\n",
      "Uploaded Part 5\n",
      "Uploaded Part 6\n",
      "Multipart Upload Completed!\n"
     ]
    }
   ],
   "source": [
    "path = 's3://nyc-taxi-data-pipeline/multipart-parquet/'\n",
    "bucket_name = 'nyc-taxi-data-pipeline'\n",
    "file_path = '/mount_folder/alpha/multipart_s3_upload/merged_file.parquet'\n",
    "s3_key = 'multipart-parquet/Multipart.parquet'\n",
    "\n",
    "response = s3.create_multipart_upload(Bucket=bucket_name, Key=s3_key)\n",
    "upload_id = response['UploadId']\n",
    "print(f\"Upload ID: {upload_id}\")\n",
    "\n",
    "chunk_size = 50 * 1024 * 1024  # 50MB per part\n",
    "parts = []\n",
    "\n",
    "with open(file_path, 'rb') as f:\n",
    "    part_number = 1\n",
    "    while True:\n",
    "        data = f.read(chunk_size)\n",
    "        if not data:\n",
    "            break\n",
    "        response = s3.upload_part(\n",
    "            Bucket=bucket_name,\n",
    "            Key=s3_key,\n",
    "            PartNumber=part_number,\n",
    "            UploadId=upload_id,\n",
    "            Body=data\n",
    "        )\n",
    "        parts.append({'ETag': response['ETag'], 'PartNumber': part_number})\n",
    "        print(f\"Uploaded Part {part_number}\")\n",
    "        part_number += 1\n",
    "        \n",
    "s3.complete_multipart_upload(\n",
    "    Bucket=bucket_name,\n",
    "    Key=s3_key,\n",
    "    UploadId=upload_id,\n",
    "    MultipartUpload={'Parts': parts}\n",
    ")\n",
    "print(\"Multipart Upload Completed!\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
