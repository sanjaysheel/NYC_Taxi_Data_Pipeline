{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Employee Data Processing with PySpark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import col, when, lit, to_date, expr, col, add_months, array, avg, collect_list, collect_set\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.types import StringType, IntegerType, DoubleType, BooleanType, StructField, StructType"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For **58 KB of data**, the processing requirements are very minimal. You don't need a large Spark cluster or heavy resource allocation. Here's the **optimal Spark configuration** for efficiently handling such a small dataset:\n",
    "\n",
    "---\n",
    "\n",
    "### **ðŸ“Œ Recommended Spark Config for 58 KB Data**\n",
    "```bash\n",
    "spark-submit \\\n",
    "  --conf spark.executor.instances=1 \\\n",
    "  --conf spark.executor.cores=1 \\\n",
    "  --conf spark.executor.memory=512m \\\n",
    "  --conf spark.executor.memoryOverhead=256m \\\n",
    "  --conf spark.driver.memory=1g \\\n",
    "  --conf spark.sql.shuffle.partitions=2 \\\n",
    "  --conf spark.dynamicAllocation.enabled=false\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "### **ðŸ“– Explanation of Configurations:**\n",
    "| **Parameter** | **Value** | **Why?** |\n",
    "|--------------|----------|----------|\n",
    "| `spark.executor.instances=1` | 1 Executor | You donâ€™t need multiple executors for such a small dataset. |\n",
    "| `spark.executor.cores=1` | 1 Core | Single-core processing is sufficient for 58 KB. |\n",
    "| `spark.executor.memory=512m` | 512MB Memory | More than enough memory to process 58 KB. |\n",
    "| `spark.executor.memoryOverhead=256m` | 256MB | Small overhead for additional tasks. |\n",
    "| `spark.driver.memory=1g` | 1GB | Since Spark operates in local mode, the driver needs some memory. |\n",
    "| `spark.sql.shuffle.partitions=2` | 2 Partitions | Default is 200, which is overkill for 58 KB; reducing to 2 speeds up processing. |\n",
    "| `spark.dynamicAllocation.enabled=false` | Disabled | No need for dynamic scaling for small data. |\n",
    "\n",
    "---\n",
    "\n",
    "### **ðŸ”¥ Additional Optimizations**\n",
    "- **Run in local mode** (since data is tiny):\n",
    "  ```bash\n",
    "  spark-submit --master local[1] <your_script.py>\n",
    "  ```\n",
    "  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Ingestion\n",
    "- Load employee data from CSV/Parquet.\n",
    "- Define schema using StructType.\n",
    "- Read the data into a Spark DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "# creating spark session for this application\n",
    "\n",
    "spark = SparkSession.builder.appName(\"Employeee\").config(\"spark.master\", \"local[*]\").config(\"spark.executor.instances\", \"1\").config(\"spark.driver.memory\",\"512m\").config(\"spark.executor.core\",\"1\").config(\"spark.executor.memoryOverhead\",\"256m\").config(\"spark.sql.shuffle.partitions\",\"2\").config(\"spark.dynamicAllocation.enabled\",\"false\").getOrCreate()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "#define the srcture of the columns\n",
    "schema = StructType([\n",
    "    StructField(\"First_Name\", StringType(), True),\n",
    "    StructField(\"Gender\", StringType(), True),\n",
    "    StructField(\"Start_Date\", StringType(), True),\n",
    "    StructField(\"Last_login_Time\", StringType(), True),\n",
    "    StructField(\"Salary\", IntegerType(), True),\n",
    "    StructField(\"Bonus\", DoubleType(), True),\n",
    "    StructField(\"Senior_Management\", BooleanType(), True),\n",
    "    StructField(\"Team\", StringType(), True),\n",
    "])\n",
    "df = spark.read.format(\"csv\").option('header', \"true\").schema(schema).load(\"/mount_folder/alpha/NYC_Taxi_Data_Pipeline_git/Practice/pyspark_functions/Employeee/employees.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+------+----------+---------------+------+------+-----------------+--------------------+\n",
      "|First_Name|Gender|Start_Date|Last_login_Time|Salary| Bonus|Senior_Management|                Team|\n",
      "+----------+------+----------+---------------+------+------+-----------------+--------------------+\n",
      "|   Douglas|  Male|  8/6/1993|       12:42 PM| 97308| 6.945|             true|           Marketing|\n",
      "|    Thomas|  Male| 3/31/1996|        6:53 AM| 61933|  4.17|             true|                null|\n",
      "|     Maria|Female| 4/23/1993|       11:17 AM|130590|11.858|            false|             Finance|\n",
      "|     Jerry|  Male|  3/4/2005|        1:00 PM|138705|  9.34|             true|             Finance|\n",
      "|     Larry|  Male| 1/24/1998|        4:47 PM|101004| 1.389|             true|     Client Services|\n",
      "|    Dennis|  Male| 4/18/1987|        1:35 AM|115163|10.125|            false|               Legal|\n",
      "|      Ruby|Female| 8/17/1987|        4:20 PM| 65476|10.012|             true|             Product|\n",
      "|      null|Female| 7/20/2015|       10:43 AM| 45906|11.598|             null|             Finance|\n",
      "|    Angela|Female|11/22/2005|        6:29 AM| 95570|18.523|             true|         Engineering|\n",
      "|   Frances|Female|  8/8/2002|        6:51 AM|139852| 7.524|             true|Business Development|\n",
      "|    Louise|Female| 8/12/1980|        9:01 AM| 63241|15.132|             true|                null|\n",
      "|     Julie|Female|10/26/1997|        3:19 PM|102508|12.637|             true|               Legal|\n",
      "|   Brandon|  Male| 12/1/1980|        1:08 AM|112807|17.492|             true|     Human Resources|\n",
      "|      Gary|  Male| 1/27/2008|       11:40 PM|109831| 5.831|            false|               Sales|\n",
      "|  Kimberly|Female| 1/14/1999|        7:13 AM| 41426|14.543|             true|             Finance|\n",
      "|   Lillian|Female|  6/5/2016|        6:09 AM| 59414| 1.256|            false|             Product|\n",
      "|    Jeremy|  Male| 9/21/2010|        5:56 AM| 90370| 7.369|            false|     Human Resources|\n",
      "|     Shawn|  Male| 12/7/1986|        7:45 PM|111737| 6.414|            false|             Product|\n",
      "|     Diana|Female|10/23/1981|       10:27 AM|132940|19.082|            false|     Client Services|\n",
      "|     Donna|Female| 7/22/2010|        3:48 AM| 81014| 1.894|            false|             Product|\n",
      "+----------+------+----------+---------------+------+------+-----------------+--------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/02/05 05:29:39 WARN CSVHeaderChecker: CSV header does not conform to the schema.\n",
      " Header: First Name, Gender, Start Date, Last Login Time, Salary, Bonus %, Senior Management, Team\n",
      " Schema: First_Name, Gender, Start_Date, Last_login_Time, Salary, Bonus, Senior_Management, Team\n",
      "Expected: First_Name but found: First Name\n",
      "CSV file: file:///mount_folder/alpha/NYC_Taxi_Data_Pipeline_git/Practice/pyspark_functions/Employeee/employees.csv\n"
     ]
    }
   ],
   "source": [
    "df.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Cleaning & Transformation\n",
    "- Convert Start Date and Last Login Time to proper date formats.\n",
    "- Handle missing values (fillna for Salary, Bonus %, etc.).\n",
    "- Remove duplicates."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert date columns to DateType\n",
    "\n",
    "# ===================== about to_date function\n",
    "# Converts a ~pyspark.sql.Column into pyspark.sql.types.DateType \n",
    "# using the optionally specified format. Specify formats according to datetime pattern_.\n",
    "# By default, it follows casting rules to pyspark.sql.types.DateType if the format is omitted. \n",
    "# Equivalent to col.cast(\"date\").\n",
    "\n",
    "df = df.withColumn(\"Formatted_Start_Date\", to_date(df[\"Start_Date\"], \"M/d/yyyy\")).fillna({\"Bonus\":0.0,\"Salary\":0, \"Team\":\"Unknown\", \"Senior_Management\":False})\n",
    "df = df.drop_duplicates(list(df.columns))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature Engineering\n",
    "- Calculate Total Earnings (Salary + Bonus).\n",
    "- Add a Probation End Date (Start Date + 3 months).\n",
    "- Generate Full Name as an array column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df.withColumn(\"Total_Earning\", expr(\"Salary + Bonus\")).show()\n",
    "# or\n",
    "df = df.withColumn(\"Total_Earning\", col(\"Salary\") + (col(\"Salary\") * col(\"Bonus\") / 100))\n",
    "df = df.withColumn(\"last_Probation\", add_months(to_date(df['start_date'], 'M/d/yyyy'), 3))\n",
    "df = df.withColumn(\"Full_name\", array(\"First_Name\", \"Team\"))\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Aggregation\n",
    "- Calculate average salary per team.\n",
    "- Collect team members using collect_list().\n",
    "- Collect unique team members using collect_set()."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_aggregated  = df.groupBy(\"Team\").agg(\n",
    "    avg(df['salary']).alias(\"avg_Salary\"),\n",
    "    collect_list(\"First_Name\").alias(\"Team_Members\"),\n",
    "    collect_set(\"First_Name\").alias(\"Unique_Team_Members\")\n",
    ")\n",
    "\n",
    "# df.groupBy(\"Team\").agg(avg(\"Salary\").alias(\"Avg Salary\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_final = df.join(df_aggregated, on=\"Team\", how=\"left\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Performance Optimization\n",
    "- Cache DataFrame to avoid recomputation.\n",
    "- Reduce shuffle partitions for small datasets (spark.sql.shuffle.partitions)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/02/05 05:29:39 WARN CacheManager: Asked to cache already cached data.\n"
     ]
    }
   ],
   "source": [
    "# Cache the DataFrame to optimize repeated operations\n",
    "df_final.cache()\n",
    "\n",
    "# Reduce shuffle partitions for small data\n",
    "spark.conf.set(\"spark.sql.shuffle.partitions\", \"2\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Unit Testing for Data Quality\n",
    "- Check for duplicate records.\n",
    "- Check for null values in key columns.\n",
    "- Ensure salary is non-negative."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Team',\n",
       " 'First_Name',\n",
       " 'Gender',\n",
       " 'Start_Date',\n",
       " 'Last_login_Time',\n",
       " 'Salary',\n",
       " 'Bonus',\n",
       " 'Senior_Management',\n",
       " 'Formatted_Start_Date',\n",
       " 'Total_Earning',\n",
       " 'last_Probation',\n",
       " 'Full_name',\n",
       " 'avg_Salary',\n",
       " 'Team_Members',\n",
       " 'Unique_Team_Members']"
      ]
     },
     "execution_count": 98,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_final.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_final = df_final.fillna({\"First_Name\": \"Unknown\", \"avg_Salary\": 0})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1000"
      ]
     },
     "execution_count": 107,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_final.count()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/02/05 05:31:14 WARN CSVHeaderChecker: CSV header does not conform to the schema.\n",
      " Header: First Name, Gender, Start Date, Last Login Time, Salary, Bonus %, Senior Management, Team\n",
      " Schema: First_Name, Gender, Start_Date, Last_login_Time, Salary, Bonus, Senior_Management, Team\n",
      "Expected: First_Name but found: First Name\n",
      "CSV file: file:///mount_folder/alpha/NYC_Taxi_Data_Pipeline_git/Practice/pyspark_functions/Employeee/employees.csv\n"
     ]
    }
   ],
   "source": [
    "# Check for duplicate records\n",
    "df_duplicates = df_final.groupBy(df_final.columns).count().filter(col(\"count\") > 1)\n",
    "assert df_duplicates.count() == 0, \"Duplicates found in data!\"\n",
    "\n",
    "# Check for null values in key columns\n",
    "assert df_final.filter(col(\"First_Name\").isNull()).count() == 0, \"Null values found in First Name!\"\n",
    "assert df.filter(col(\"Salary\").isNull()).count() == 0, \"Null values found in Salary!\"\n",
    "\n",
    "# # Check if salary is non-negative\n",
    "assert df.filter(col(\"Salary\") < 0).count() == 0, \"Negative salaries found!\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Save Processed Data\n",
    "- Write processed data to Parquet format."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/02/05 05:31:25 WARN CSVHeaderChecker: CSV header does not conform to the schema.\n",
      " Header: First Name, Gender, Start Date, Last Login Time, Salary, Bonus %, Senior Management, Team\n",
      " Schema: First_Name, Gender, Start_Date, Last_login_Time, Salary, Bonus, Senior_Management, Team\n",
      "Expected: First_Name but found: First Name\n",
      "CSV file: file:///mount_folder/alpha/NYC_Taxi_Data_Pipeline_git/Practice/pyspark_functions/Employeee/employees.csv\n",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "df.repartition(1).write.format(\"parquet\").mode(\"overwrite\").save(\"/mount_folder/alpha/NYC_Taxi_Data_Pipeline_git/Practice/pyspark_functions/Employeee/employees_processed.parquet\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
