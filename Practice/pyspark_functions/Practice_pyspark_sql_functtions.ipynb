{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Employee Data Processing with PySpark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import col, when, lit, to_date, expr, col, add_months, array, avg, collect_list, collect_set\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.types import StringType, IntegerType, DoubleType, BooleanType, StructField, StructType"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For **58 KB of data**, the processing requirements are very minimal. You don't need a large Spark cluster or heavy resource allocation. Here's the **optimal Spark configuration** for efficiently handling such a small dataset:\n",
    "\n",
    "---\n",
    "\n",
    "### **ðŸ“Œ Recommended Spark Config for 58 KB Data**\n",
    "```bash\n",
    "spark-submit \\\n",
    "  --conf spark.executor.instances=1 \\\n",
    "  --conf spark.executor.cores=1 \\\n",
    "  --conf spark.executor.memory=512m \\\n",
    "  --conf spark.executor.memoryOverhead=256m \\\n",
    "  --conf spark.driver.memory=1g \\\n",
    "  --conf spark.sql.shuffle.partitions=2 \\\n",
    "  --conf spark.dynamicAllocation.enabled=false\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "### **ðŸ“– Explanation of Configurations:**\n",
    "| **Parameter** | **Value** | **Why?** |\n",
    "|--------------|----------|----------|\n",
    "| `spark.executor.instances=1` | 1 Executor | You donâ€™t need multiple executors for such a small dataset. |\n",
    "| `spark.executor.cores=1` | 1 Core | Single-core processing is sufficient for 58 KB. |\n",
    "| `spark.executor.memory=512m` | 512MB Memory | More than enough memory to process 58 KB. |\n",
    "| `spark.executor.memoryOverhead=256m` | 256MB | Small overhead for additional tasks. |\n",
    "| `spark.driver.memory=1g` | 1GB | Since Spark operates in local mode, the driver needs some memory. |\n",
    "| `spark.sql.shuffle.partitions=2` | 2 Partitions | Default is 200, which is overkill for 58 KB; reducing to 2 speeds up processing. |\n",
    "| `spark.dynamicAllocation.enabled=false` | Disabled | No need for dynamic scaling for small data. |\n",
    "\n",
    "---\n",
    "\n",
    "### **ðŸ”¥ Additional Optimizations**\n",
    "- **Run in local mode** (since data is tiny):\n",
    "  ```bash\n",
    "  spark-submit --master local[1] <your_script.py>\n",
    "  ```\n",
    "  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Ingestion\n",
    "- Load employee data from CSV/Parquet.\n",
    "- Define schema using StructType.\n",
    "- Read the data into a Spark DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# creating spark session for this application\n",
    "\n",
    "spark = SparkSession.builder.appName(\"Employeee\").config(\"spark.master\", \"local[*]\").config(\"spark.executor.instances\", \"1\").config(\"spark.driver.memory\",\"512m\").config(\"spark.executor.core\",\"1\").config(\"spark.executor.memoryOverhead\",\"256m\").config(\"spark.sql.shuffle.partitions\",\"2\").config(\"spark.dynamicAllocation.enabled\",\"false\").getOrCreate()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "#define the srcture of the columns\n",
    "schema = StructType([\n",
    "    StructField(\"First_Name\", StringType(), True),\n",
    "    StructField(\"Gender\", StringType(), True),\n",
    "    StructField(\"Start_Date\", StringType(), True),\n",
    "    StructField(\"Last_login_Time\", StringType(), True),\n",
    "    StructField(\"Salary\", IntegerType(), True),\n",
    "    StructField(\"Bonus\", DoubleType(), True),\n",
    "    StructField(\"Senior_Management\", BooleanType(), True),\n",
    "    StructField(\"Team\", StringType(), True),\n",
    "])\n",
    "df = spark.read.format(\"csv\").option('header', \"true\").schema(schema).load(\"/mount_folder/alpha/NYC_Taxi_Data_Pipeline_git/Practice/pyspark_functions/Employeee/employees.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+------+----------+---------------+------+------+-----------------+--------------------+\n",
      "|First_Name|Gender|Start_Date|Last_login_Time|Salary| Bonus|Senior_Management|                Team|\n",
      "+----------+------+----------+---------------+------+------+-----------------+--------------------+\n",
      "|   Douglas|  Male|  8/6/1993|       12:42 PM| 97308| 6.945|             true|           Marketing|\n",
      "|    Thomas|  Male| 3/31/1996|        6:53 AM| 61933|  4.17|             true|                null|\n",
      "|     Maria|Female| 4/23/1993|       11:17 AM|130590|11.858|            false|             Finance|\n",
      "|     Jerry|  Male|  3/4/2005|        1:00 PM|138705|  9.34|             true|             Finance|\n",
      "|     Larry|  Male| 1/24/1998|        4:47 PM|101004| 1.389|             true|     Client Services|\n",
      "|    Dennis|  Male| 4/18/1987|        1:35 AM|115163|10.125|            false|               Legal|\n",
      "|      Ruby|Female| 8/17/1987|        4:20 PM| 65476|10.012|             true|             Product|\n",
      "|      null|Female| 7/20/2015|       10:43 AM| 45906|11.598|             null|             Finance|\n",
      "|    Angela|Female|11/22/2005|        6:29 AM| 95570|18.523|             true|         Engineering|\n",
      "|   Frances|Female|  8/8/2002|        6:51 AM|139852| 7.524|             true|Business Development|\n",
      "|    Louise|Female| 8/12/1980|        9:01 AM| 63241|15.132|             true|                null|\n",
      "|     Julie|Female|10/26/1997|        3:19 PM|102508|12.637|             true|               Legal|\n",
      "|   Brandon|  Male| 12/1/1980|        1:08 AM|112807|17.492|             true|     Human Resources|\n",
      "|      Gary|  Male| 1/27/2008|       11:40 PM|109831| 5.831|            false|               Sales|\n",
      "|  Kimberly|Female| 1/14/1999|        7:13 AM| 41426|14.543|             true|             Finance|\n",
      "|   Lillian|Female|  6/5/2016|        6:09 AM| 59414| 1.256|            false|             Product|\n",
      "|    Jeremy|  Male| 9/21/2010|        5:56 AM| 90370| 7.369|            false|     Human Resources|\n",
      "|     Shawn|  Male| 12/7/1986|        7:45 PM|111737| 6.414|            false|             Product|\n",
      "|     Diana|Female|10/23/1981|       10:27 AM|132940|19.082|            false|     Client Services|\n",
      "|     Donna|Female| 7/22/2010|        3:48 AM| 81014| 1.894|            false|             Product|\n",
      "+----------+------+----------+---------------+------+------+-----------------+--------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/02/05 04:59:07 WARN CSVHeaderChecker: CSV header does not conform to the schema.\n",
      " Header: First Name, Gender, Start Date, Last Login Time, Salary, Bonus %, Senior Management, Team\n",
      " Schema: First_Name, Gender, Start_Date, Last_login_Time, Salary, Bonus, Senior_Management, Team\n",
      "Expected: First_Name but found: First Name\n",
      "CSV file: file:///mount_folder/alpha/NYC_Taxi_Data_Pipeline_git/Practice/pyspark_functions/Employeee/employees.csv\n"
     ]
    }
   ],
   "source": [
    "df.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Cleaning & Transformation\n",
    "- Convert Start Date and Last Login Time to proper date formats.\n",
    "- Handle missing values (fillna for Salary, Bonus %, etc.).\n",
    "- Remove duplicates."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert date columns to DateType\n",
    "\n",
    "# ===================== about to_date function\n",
    "# Converts a ~pyspark.sql.Column into pyspark.sql.types.DateType \n",
    "# using the optionally specified format. Specify formats according to datetime pattern_.\n",
    "# By default, it follows casting rules to pyspark.sql.types.DateType if the format is omitted. \n",
    "# Equivalent to col.cast(\"date\").\n",
    "\n",
    "df = df.withColumn(\"Formatted_Start_Date\", to_date(df[\"Start_Date\"], \"M/d/yyyy\")).fillna({\"Bonus\":0.0,\"Salary\":0, \"Team\":\"Unknown\", \"Senior_Management\":False})\n",
    "df = df.drop_duplicates(list(df.columns))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature Engineering\n",
    "- Calculate Total Earnings (Salary + Bonus).\n",
    "- Add a Probation End Date (Start Date + 3 months).\n",
    "- Generate Full Name as an array column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df.withColumn(\"Total_Earning\", expr(\"Salary + Bonus\")).show()\n",
    "# or\n",
    "df = df.withColumn(\"Total_Earning\", col(\"Salary\") + (col(\"Salary\") * col(\"Bonus\") / 100))\n",
    "df = df.withColumn(\"last_Probation\", add_months(to_date(df['start_date'], 'M/d/yyyy'), 3))\n",
    "df = df.withColumn(\"Full_name\", array(\"First_Name\", \"Team\"))\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Aggregation\n",
    "- Calculate average salary per team.\n",
    "- Collect team members using collect_list().\n",
    "- Collect unique team members using collect_set()."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_aggregated  = df.groupBy(\"Team\").agg(\n",
    "    avg(df['salary']).alias(\"avg_Salary\"),\n",
    "    collect_list(\"First_Name\").alias(\"Team_Members\"),\n",
    "    collect_set(\"First_Name\").alias(\"Unique_Team_Members\")\n",
    ")\n",
    "\n",
    "# df.groupBy(\"Team\").agg(avg(\"Salary\").alias(\"Avg Salary\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_final = df.join(df_aggregated, on=\"Team\", how=\"left\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Performance Optimization\n",
    "- Cache DataFrame to avoid recomputation.\n",
    "- Reduce shuffle partitions for small datasets (spark.sql.shuffle.partitions)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/02/05 04:59:07 WARN CacheManager: Asked to cache already cached data.\n"
     ]
    }
   ],
   "source": [
    "# Cache the DataFrame to optimize repeated operations\n",
    "df_final.cache()\n",
    "\n",
    "# Reduce shuffle partitions for small data\n",
    "spark.conf.set(\"spark.sql.shuffle.partitions\", \"2\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Unit Testing for Data Quality\n",
    "- Check for duplicate records.\n",
    "- Check for null values in key columns.\n",
    "- Ensure salary is non-negative."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Team',\n",
       " 'First_Name',\n",
       " 'Gender',\n",
       " 'Start_Date',\n",
       " 'Last_login_Time',\n",
       " 'Salary',\n",
       " 'Bonus',\n",
       " 'Senior_Management',\n",
       " 'Formatted_Start_Date',\n",
       " 'Total_Earning',\n",
       " 'last_Probation',\n",
       " 'Full_name',\n",
       " 'avg_Salary',\n",
       " 'Team_Members',\n",
       " 'Unique_Team_Members']"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_final.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/02/05 04:59:48 WARN CSVHeaderChecker: CSV header does not conform to the schema.\n",
      " Header: First Name, Gender, Start Date, Last Login Time, Salary, Bonus %, Senior Management, Team\n",
      " Schema: First_Name, Gender, Start_Date, Last_login_Time, Salary, Bonus, Senior_Management, Team\n",
      "Expected: First_Name but found: First Name\n",
      "CSV file: file:///mount_folder/alpha/NYC_Taxi_Data_Pipeline_git/Practice/pyspark_functions/Employeee/employees.csv\n",
      "25/02/05 04:59:48 WARN CSVHeaderChecker: CSV header does not conform to the schema.\n",
      " Header: First Name, Gender, Start Date, Last Login Time, Salary, Bonus %, Senior Management, Team\n",
      " Schema: First_Name, Gender, Start_Date, Last_login_Time, Salary, Bonus, Senior_Management, Team\n",
      "Expected: First_Name but found: First Name\n",
      "CSV file: file:///mount_folder/alpha/NYC_Taxi_Data_Pipeline_git/Practice/pyspark_functions/Employeee/employees.csv\n"
     ]
    },
    {
     "ename": "AssertionError",
     "evalue": "Null values found in First Name!",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAssertionError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[44], line 6\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m df_duplicates\u001b[38;5;241m.\u001b[39mcount() \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mDuplicates found in data!\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m      5\u001b[0m \u001b[38;5;66;03m# Check for null values in key columns\u001b[39;00m\n\u001b[0;32m----> 6\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m df_final\u001b[38;5;241m.\u001b[39mfilter(col(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFirst_Name\u001b[39m\u001b[38;5;124m\"\u001b[39m)\u001b[38;5;241m.\u001b[39misNull())\u001b[38;5;241m.\u001b[39mcount() \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNull values found in First Name!\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m      7\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m df\u001b[38;5;241m.\u001b[39mfilter(col(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mSalary\u001b[39m\u001b[38;5;124m\"\u001b[39m)\u001b[38;5;241m.\u001b[39misNull())\u001b[38;5;241m.\u001b[39mcount() \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNull values found in Salary!\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m      9\u001b[0m \u001b[38;5;66;03m# # Check if salary is non-negative\u001b[39;00m\n",
      "\u001b[0;31mAssertionError\u001b[0m: Null values found in First Name!"
     ]
    }
   ],
   "source": [
    "# Check for duplicate records\n",
    "df_duplicates = df_final.groupBy(df_final.columns).count().filter(col(\"count\") > 1)\n",
    "assert df_duplicates.count() == 0, \"Duplicates found in data!\"\n",
    "\n",
    "# Check for null values in key columns\n",
    "assert df_final.filter(col(\"First_Name\").isNull()).count() == 0, \"Null values found in First Name!\"\n",
    "assert df.filter(col(\"Salary\").isNull()).count() == 0, \"Null values found in Salary!\"\n",
    "\n",
    "# # Check if salary is non-negative\n",
    "assert df.filter(col(\"Salary\") < 0).count() == 0, \"Negative salaries found!\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+------+----------+---------------+------+------+-----------------+--------------------+--------------------+-------------+--------------+--------------------+\n",
      "|First_Name|Gender|Start_Date|Last_login_Time|Salary| Bonus|Senior_Management|                Team|Formatted_Start_Date|Total_Earning|last_Probation|           Full_name|\n",
      "+----------+------+----------+---------------+------+------+-----------------+--------------------+--------------------+-------------+--------------+--------------------+\n",
      "|      null|Female| 7/20/2015|       10:43 AM| 45906|11.598|            false|             finance|          2015-07-20|  51230.17788|    2015-10-20|     [null, Finance]|\n",
      "|      null|  Male| 8/21/1998|        2:27 PM|122340| 6.417|            false|             unknown|          1998-08-21|  130190.5578|    1998-11-21|     [null, Unknown]|\n",
      "|      null|  Male| 1/29/2016|        2:33 AM|122173| 7.797|            false|     client services|          2016-01-29| 131698.82881|    2016-04-29|[null, Client Ser...|\n",
      "|      null|Female| 6/12/2007|        5:25 PM| 58112|19.414|            false|           marketing|          2007-06-12|  69393.86368|    2007-09-12|   [null, Marketing]|\n",
      "|      null|  Male| 6/22/1991|        8:58 PM| 76189|18.988|            false|               legal|          1991-06-22|  90655.76732|    1991-09-22|       [null, Legal]|\n",
      "|      null|Female| 10/3/1990|        1:08 AM|132373|10.527|            false|             unknown|          1990-10-03| 146307.90571|    1991-01-03|     [null, Unknown]|\n",
      "|      null|Female| 8/17/2014|        2:00 PM| 86230| 8.578|            false|        distribution|          2014-08-17|   93626.8094|    2014-11-17|[null, Distribution]|\n",
      "|      null|  null| 9/18/2007|        6:59 PM| 40297| 6.185|            false|     client services|          2007-09-18|  42789.36945|    2007-12-18|[null, Client Ser...|\n",
      "|      null|Female| 2/23/2005|        9:50 PM|149654| 1.825|            false|               sales|          2005-02-23|  152385.1855|    2005-05-23|       [null, Sales]|\n",
      "|      null|  null| 4/28/2003|        3:01 AM|113732|10.391|            false|               sales|          2003-04-28| 125549.89212|    2003-07-28|       [null, Sales]|\n",
      "|      null|  Male|11/25/1999|        6:16 AM| 42676|15.517|            false|               sales|          1999-11-25|  49298.03492|    2000-02-25|       [null, Sales]|\n",
      "|      null|Female|  8/2/1992|        8:35 PM|145316|18.517|            false|     human resources|          1992-08-02| 172224.16372|    1992-11-02|[null, Human Reso...|\n",
      "|      null|Female|  7/8/2008|       11:40 PM| 62960|14.356|            false|               sales|          2008-07-08|   71998.5376|    2008-10-08|       [null, Sales]|\n",
      "|      null|  Male| 1/27/2010|        2:57 AM| 87760|14.987|            false|         engineering|          2010-01-27|  100912.5912|    2010-04-27| [null, Engineering]|\n",
      "|      null|  Male| 10/9/2011|        9:29 AM| 69906| 4.844|            false|         engineering|          2011-10-09|  73292.24664|    2012-01-09| [null, Engineering]|\n",
      "|      null|Female|12/31/2006|       11:02 PM| 81444| 3.171|            false|        distribution|          2006-12-31|  84026.58924|    2007-03-31|[null, Distribution]|\n",
      "|      null|Female| 9/18/2002|       12:39 PM|118906| 4.537|            false|           marketing|          2002-09-18| 124300.76522|    2002-12-18|   [null, Marketing]|\n",
      "|      null|  Male| 6/29/2008|        5:23 AM|111043| 5.966|            false|        distribution|          2008-06-29| 117667.82538|    2008-09-29|[null, Distribution]|\n",
      "|      null|  Male| 5/24/2016|        9:17 PM| 76409| 7.008|            false|        distribution|          2016-05-24|  81763.74272|    2016-08-24|[null, Distribution]|\n",
      "|      null|Female| 4/27/2013|        6:40 AM| 93847| 1.085|            false|business development|          2013-04-27|  94865.23995|    2013-07-27|[null, Business D...|\n",
      "+----------+------+----------+---------------+------+------+-----------------+--------------------+--------------------+-------------+--------------+--------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/02/05 05:00:39 WARN CSVHeaderChecker: CSV header does not conform to the schema.\n",
      " Header: First Name, Gender, Start Date, Last Login Time, Salary, Bonus %, Senior Management, Team\n",
      " Schema: First_Name, Gender, Start_Date, Last_login_Time, Salary, Bonus, Senior_Management, Team\n",
      "Expected: First_Name but found: First Name\n",
      "CSV file: file:///mount_folder/alpha/NYC_Taxi_Data_Pipeline_git/Practice/pyspark_functions/Employeee/employees.csv\n"
     ]
    }
   ],
   "source": [
    "df.filter(col(\"First_Name\").isNull()).show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/02/05 05:26:31 WARN CSVHeaderChecker: CSV header does not conform to the schema.\n",
      " Header: First Name, Gender, Start Date, Last Login Time, Salary, Bonus %, Senior Management, Team\n",
      " Schema: First_Name, Gender, Start_Date, Last_login_Time, Salary, Bonus, Senior_Management, Team\n",
      "Expected: First_Name but found: First Name\n",
      "CSV file: file:///mount_folder/alpha/NYC_Taxi_Data_Pipeline_git/Practice/pyspark_functions/Employeee/employees.csv\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+-----------------+\n",
      "|                Team|       Avg_Salary|\n",
      "+--------------------+-----------------+\n",
      "|             unknown|90763.13953488372|\n",
      "|             finance|92219.48039215687|\n",
      "|         engineering|94269.19565217392|\n",
      "|             unknown|90763.13953488372|\n",
      "|             finance|92219.48039215687|\n",
      "|             product| 88665.5052631579|\n",
      "|     human resources|90944.52747252748|\n",
      "|             product| 88665.5052631579|\n",
      "|             product| 88665.5052631579|\n",
      "|           marketing|90435.59183673469|\n",
      "|     client services|88224.42452830188|\n",
      "|     client services|88224.42452830188|\n",
      "|     client services|88224.42452830188|\n",
      "|               legal|89303.61363636363|\n",
      "|         engineering|94269.19565217392|\n",
      "|             product| 88665.5052631579|\n",
      "|             unknown|90763.13953488372|\n",
      "|business development|91866.31683168317|\n",
      "|     client services|88224.42452830188|\n",
      "|business development|91866.31683168317|\n",
      "+--------------------+-----------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/02/05 05:26:32 WARN CSVHeaderChecker: CSV header does not conform to the schema.\n",
      " Header: First Name, Gender, Start Date, Last Login Time, Salary, Bonus %, Senior Management, Team\n",
      " Schema: First_Name, Gender, Start_Date, Last_login_Time, Salary, Bonus, Senior_Management, Team\n",
      "Expected: First_Name but found: First Name\n",
      "CSV file: file:///mount_folder/alpha/NYC_Taxi_Data_Pipeline_git/Practice/pyspark_functions/Employeee/employees.csv\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+----------+------+----------+---------------+------+-----+-----------------+--------------------+-------------+--------------+---------+----------+------------+-------------------+\n",
      "|Team|First_Name|Gender|Start_Date|Last_login_Time|Salary|Bonus|Senior_Management|Formatted_Start_Date|Total_Earning|last_Probation|Full_name|avg_Salary|Team_Members|Unique_Team_Members|\n",
      "+----+----------+------+----------+---------------+------+-----+-----------------+--------------------+-------------+--------------+---------+----------+------------+-------------------+\n",
      "+----+----------+------+----------+---------------+------+-----+-----------------+--------------------+-------------+--------------+---------+----------+------------+-------------------+\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "DataFrame[Team: string, First_Name: string, Gender: string, Start_Date: string, Last_login_Time: string, Salary: int, Bonus: double, Senior_Management: boolean, Formatted_Start_Date: date, Total_Earning: double, last_Probation: date, Full_name: array<string>, avg_Salary: double, Team_Members: array<string>, Unique_Team_Members: array<string>]"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = df.filter(col(\"Team\").isNotNull())\n",
    "df_aggregated = df_aggregated.filter(col(\"Team\").isNotNull())\n",
    "from pyspark.sql.functions import trim, lower\n",
    "\n",
    "df = df.withColumn(\"Team\", trim(lower(col(\"Team\"))))\n",
    "df_aggregated = df_aggregated.withColumn(\"Team\", trim(lower(col(\"Team\"))))\n",
    "df_final = df.join(df_aggregated, on=\"Team\", how=\"left\")\n",
    "df_final.select(\"Team\", \"Avg_Salary\").show()\n",
    "df_final.filter(col(\"Avg_Salary\").isNull()).show()\n",
    "df_final.dropDuplicates()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/02/05 05:16:31 WARN CSVHeaderChecker: CSV header does not conform to the schema.\n",
      " Header: First Name, Gender, Start Date, Last Login Time, Salary, Bonus %, Senior Management, Team\n",
      " Schema: First_Name, Gender, Start_Date, Last_login_Time, Salary, Bonus, Senior_Management, Team\n",
      "Expected: First_Name but found: First Name\n",
      "CSV file: file:///mount_folder/alpha/NYC_Taxi_Data_Pipeline_git/Practice/pyspark_functions/Employeee/employees.csv\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "1000"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.count()\n",
    "# df_final.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<bound method DataFrame.printSchema of DataFrame[Team: string, First_Name: string, Gender: string, Start_Date: string, Last_login_Time: string, Salary: int, Bonus: double, Senior_Management: boolean, Formatted_Start_Date: date, Total_Earning: double, last_Probation: date, Full_name: array<string>, avg_Salary: double, Team_Members: array<string>, Unique_Team_Members: array<string>]>"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_final.printSchema"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Save Processed Data\n",
    "- Write processed data to Parquet format."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'df' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[3], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mdf\u001b[49m\u001b[38;5;241m.\u001b[39mrepartition(\u001b[38;5;241m1\u001b[39m)\u001b[38;5;241m.\u001b[39mwrite\u001b[38;5;241m.\u001b[39mformat(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mparquet\u001b[39m\u001b[38;5;124m\"\u001b[39m)\u001b[38;5;241m.\u001b[39mmode(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124moverwrite\u001b[39m\u001b[38;5;124m\"\u001b[39m)\u001b[38;5;241m.\u001b[39msave(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m/mount_folder/alpha/NYC_Taxi_Data_Pipeline_git/Practice/pyspark_functions/Employeee/employees_processed.parquet\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'df' is not defined"
     ]
    }
   ],
   "source": [
    "# df.repartition(1).write.format(\"parquet\").mode(\"overwrite\").save(\"/mount_folder/alpha/NYC_Taxi_Data_Pipeline_git/Practice/pyspark_functions/Employeee/employees_processed.parquet\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
