{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# updated"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Got it! I'll now structure the **Employee Data Processing with PySpark** project into a **Jupyter Notebook (`.ipynb`) format**, incorporating all the **Spark SQL functions** you requested.  \n",
    "\n",
    "### **🚀 Project: Employee Data Processing with PySpark**\n",
    "We'll cover the following:  \n",
    "✅ **Data Ingestion** (CSV/Parquet)  \n",
    "✅ **Data Cleaning & Transformation**  \n",
    "✅ **Data Aggregation**  \n",
    "✅ **Feature Engineering**  \n",
    "✅ **Performance Optimization (Spark Config Tuning)**  \n",
    "✅ **Unit Testing for Data Quality**  \n",
    "\n",
    "---\n",
    "### **📌 1. Setup PySpark in Jupyter Notebook**\n",
    "```python\n",
    "# Install PySpark if not installed (Only needed once)\n",
    "!pip install pyspark\n",
    "```\n",
    "\n",
    "```python\n",
    "# Import Required Libraries\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import *\n",
    "from pyspark.sql.types import *\n",
    "```\n",
    "\n",
    "```python\n",
    "# Create Spark Session\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"Employee Data Processing\") \\\n",
    "    .config(\"spark.executor.memory\", \"2g\") \\\n",
    "    .config(\"spark.driver.memory\", \"2g\") \\\n",
    "    .config(\"spark.sql.shuffle.partitions\", \"4\") \\\n",
    "    .getOrCreate()\n",
    "```\n",
    "\n",
    "---\n",
    "### **📌 2. Load Employee Data**\n",
    "```python\n",
    "# Define schema\n",
    "schema = StructType([\n",
    "    StructField(\"First Name\", StringType(), True),\n",
    "    StructField(\"Gender\", StringType(), True),\n",
    "    StructField(\"Start Date\", StringType(), True),\n",
    "    StructField(\"Last Login Time\", StringType(), True),\n",
    "    StructField(\"Salary\", IntegerType(), True),\n",
    "    StructField(\"Bonus %\", DoubleType(), True),\n",
    "    StructField(\"Senior Management\", BooleanType(), True),\n",
    "    StructField(\"Team\", StringType(), True)\n",
    "])\n",
    "\n",
    "# Load Data\n",
    "file_path = \"employee_data.csv\"  # Change path if needed\n",
    "df = spark.read.csv(file_path, header=True, schema=schema)\n",
    "```\n",
    "\n",
    "---\n",
    "### **📌 3. Data Cleaning & Transformation**\n",
    "```python\n",
    "# Convert date columns to DateType\n",
    "df = df.withColumn(\"Start Date\", to_date(df[\"Start Date\"], \"yyyy-MM-dd\")) \\\n",
    "       .withColumn(\"Last Login Time\", to_timestamp(df[\"Last Login Time\"], \"yyyy-MM-dd HH:mm:ss\"))\n",
    "\n",
    "# Handle missing values\n",
    "df = df.fillna({\"Salary\": 0, \"Bonus %\": 0.0, \"Senior Management\": False, \"Team\": \"Unknown\"})\n",
    "```\n",
    "\n",
    "---\n",
    "### **📌 4. Feature Engineering**\n",
    "```python\n",
    "# Calculate total earnings (Salary + Bonus)\n",
    "df = df.withColumn(\"Total Earnings\", col(\"Salary\") + (col(\"Salary\") * col(\"Bonus %\") / 100))\n",
    "\n",
    "# Add months to Start Date (Example: Probation end date)\n",
    "df = df.withColumn(\"Probation End Date\", add_months(col(\"Start Date\"), 3))\n",
    "\n",
    "# Generate employee full name as an array\n",
    "df = df.withColumn(\"Full Name\", array(\"First Name\", \"Team\"))\n",
    "```\n",
    "\n",
    "---\n",
    "### **📌 5. Data Aggregation**\n",
    "```python\n",
    "# Average salary per team\n",
    "df_avg_salary = df.groupBy(\"Team\").agg(avg(\"Salary\").alias(\"Avg Salary\"))\n",
    "\n",
    "# Collect list of employees per team\n",
    "df_team_members = df.groupBy(\"Team\").agg(collect_list(\"First Name\").alias(\"Team Members\"))\n",
    "\n",
    "# Collect set of unique employees per team\n",
    "df_unique_team_members = df.groupBy(\"Team\").agg(collect_set(\"First Name\").alias(\"Unique Team Members\"))\n",
    "```\n",
    "\n",
    "---\n",
    "### **📌 6. Performance Optimization**\n",
    "```python\n",
    "# Cache the DataFrame to optimize repeated operations\n",
    "df.cache()\n",
    "\n",
    "# Reduce shuffle partitions for small data\n",
    "spark.conf.set(\"spark.sql.shuffle.partitions\", \"2\")\n",
    "```\n",
    "\n",
    "---\n",
    "### **📌 7. Unit Testing for Data Quality**\n",
    "```python\n",
    "# Check for duplicate records\n",
    "df_duplicates = df.groupBy(df.columns).count().filter(col(\"count\") > 1)\n",
    "assert df_duplicates.count() == 0, \"Duplicates found in data!\"\n",
    "\n",
    "# Check for null values in key columns\n",
    "assert df.filter(col(\"First Name\").isNull()).count() == 0, \"Null values found in First Name!\"\n",
    "assert df.filter(col(\"Salary\").isNull()).count() == 0, \"Null values found in Salary!\"\n",
    "\n",
    "# Check if salary is non-negative\n",
    "assert df.filter(col(\"Salary\") < 0).count() == 0, \"Negative salaries found!\"\n",
    "```\n",
    "\n",
    "---\n",
    "### **📌 8. Save Processed Data**\n",
    "```python\n",
    "df.write.mode(\"overwrite\").parquet(\"processed_employee_data.parquet\")\n",
    "```\n",
    "\n",
    "---\n",
    "### **✅ Next Steps**\n",
    "- Run each cell step by step in your **Jupyter Notebook** (`.ipynb`).\n",
    "- Modify the **file path** for input/output data as needed.\n",
    "- Experiment with different **Spark SQL functions** for practice.\n",
    "\n",
    "---\n",
    "This project ensures you're **using all the essential PySpark functions** a **data engineer** works with **daily** while covering **data ingestion, transformation, feature engineering, aggregation, and optimization**.\n",
    "\n",
    "Let me know if you need any refinements! 🚀"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **JIRA Ticket: Employee Data Processing with PySpark**  \n",
    "\n",
    "**📌 Project:** Employee Data Processing with PySpark  \n",
    "**💼 Epic:** Data Pipeline Development  \n",
    "**📅 Created Date:** [Today's Date]  \n",
    "**👤 Assignee:** [Your Name]  \n",
    "**📌 Priority:** High  \n",
    "**🔁 Sprint:** [Sprint Name]  \n",
    "\n",
    "---\n",
    "\n",
    "## **🎯 Summary**  \n",
    "Build a **PySpark-based data processing pipeline** to handle **employee data**. This pipeline should cover **data ingestion, transformation, feature engineering, aggregation, performance optimization, and unit testing** in a **Jupyter Notebook (`.ipynb`)** environment.  \n",
    "\n",
    "---\n",
    "\n",
    "## **📝 Description**  \n",
    "\n",
    "### **Task Breakdown:**  \n",
    "\n",
    "### **1️⃣ Data Ingestion**  \n",
    "- Load employee data from **CSV/Parquet**.  \n",
    "- Define **schema** using `StructType`.  \n",
    "- Read the data into a **Spark DataFrame**.  \n",
    "\n",
    "### **2️⃣ Data Cleaning & Transformation**  \n",
    "- Convert **Start Date** and **Last Login Time** to proper date formats.  \n",
    "- Handle **missing values** (`fillna` for Salary, Bonus %, etc.).  \n",
    "- Remove **duplicates**.  \n",
    "\n",
    "### **3️⃣ Feature Engineering**  \n",
    "- Calculate **Total Earnings** (`Salary + Bonus`).  \n",
    "- Add a **Probation End Date** (`Start Date` + 3 months).  \n",
    "- Generate **Full Name** as an **array column**.  \n",
    "\n",
    "### **4️⃣ Data Aggregation**  \n",
    "- Calculate **average salary per team**.  \n",
    "- Collect **team members** using `collect_list()`.  \n",
    "- Collect **unique team members** using `collect_set()`.  \n",
    "\n",
    "### **5️⃣ Performance Optimization**  \n",
    "- **Cache** DataFrame to avoid recomputation.  \n",
    "- Reduce **shuffle partitions** for small datasets (`spark.sql.shuffle.partitions`).  \n",
    "\n",
    "### **6️⃣ Unit Testing for Data Quality**  \n",
    "- Check for **duplicate records**.  \n",
    "- Check for **null values** in key columns.  \n",
    "- Ensure **salary is non-negative**.  \n",
    "\n",
    "### **7️⃣ Save Processed Data**  \n",
    "- Write processed data to **Parquet format**.  \n",
    "\n",
    "---\n",
    "\n",
    "## **📌 Acceptance Criteria**  \n",
    "✅ **Employee data is loaded successfully into PySpark**.  \n",
    "✅ **Data cleaning and transformations are correctly applied**.  \n",
    "✅ **Feature engineering steps are implemented**.  \n",
    "✅ **Aggregations provide expected results**.  \n",
    "✅ **Performance optimizations are applied correctly**.  \n",
    "✅ **All unit tests pass successfully**.  \n",
    "✅ **Processed data is saved in the correct format**.  \n",
    "\n",
    "---\n",
    "\n",
    "## **⏳ Estimated Time**  \n",
    "**5-6 hours**  \n",
    "\n",
    "---\n",
    "\n",
    "## **🔗 Dependencies**  \n",
    "- PySpark environment setup  \n",
    "- Employee data file (CSV/Parquet)  \n",
    "\n",
    "---\n",
    "\n",
    "## **🛠️ Labels**  \n",
    "- `PySpark`  \n",
    "- `Data Engineering`  \n",
    "- `ETL`  \n",
    "- `Transformation`  \n",
    "- `Optimization`  \n",
    "\n",
    "---\n",
    "\n",
    "## **📌 JIRA Workflow Status**  \n",
    "**To Do** → In Progress → Code Review → Done  \n",
    "\n",
    "---\n",
    "\n",
    "This JIRA ticket will help you **implement and track** your PySpark data pipeline. 🚀 Let me know if you need any modifications! 💡"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "SLF4J: Class path contains multiple SLF4J bindings.\n",
      "SLF4J: Found binding in [jar:file:/home/glue_user/spark/jars/log4j-slf4j-impl-2.17.2.jar!/org/slf4j/impl/StaticLoggerBinder.class]\n",
      "SLF4J: Found binding in [jar:file:/home/glue_user/spark/jars/slf4j-reload4j-1.7.36.jar!/org/slf4j/impl/StaticLoggerBinder.class]\n",
      "SLF4J: Found binding in [jar:file:/home/glue_user/aws-glue-libs/jars/log4j-slf4j-impl-2.17.2.jar!/org/slf4j/impl/StaticLoggerBinder.class]\n",
      "SLF4J: Found binding in [jar:file:/home/glue_user/aws-glue-libs/jars/slf4j-reload4j-1.7.36.jar!/org/slf4j/impl/StaticLoggerBinder.class]\n",
      "SLF4J: See http://www.slf4j.org/codes.html#multiple_bindings for an explanation.\n",
      "SLF4J: Actual binding is of type [org.apache.logging.slf4j.Log4jLoggerFactory]\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "25/02/01 23:21:53 WARN Utils: Service 'SparkUI' could not bind on port 4040. Attempting port 4041.\n",
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+---+--------+-------------+----------+-------------------+-------------+------------+----------+-------------------+------+------+----------------------------------------+---+\n",
      "|Name   |Age|Salary  |City         |JoinDate  |Skills1            |Skills2      |Scores      |Phone     |Email              |Height|Weight|BinaryData                              |ID |\n",
      "+-------+---+--------+-------------+----------+-------------------+-------------+------------+----------+-------------------+------+------+----------------------------------------+---+\n",
      "|Alice  |30 |60000.5 |New York     |2020-01-15|[Python, SQL]      |[SQL, Java]  |[90, 85, 88]|null      |alice@example.com  |165.2 |60.5  |[41 6C 69 63 65 42 69 6E 61 72 79]      |1  |\n",
      "|Bob    |25 |50000.75|London       |2019-07-21|[Java, C++]        |[Python, C++]|[78, 80, 82]|1234567890|null               |170.4 |68.2  |[42 6F 62 42 69 6E 61 72 79]            |2  |\n",
      "|Charlie|35 |75000.0 |San Francisco|2018-05-10|[JavaScript, Scala]|[Scala, Rust]|[95, 92, 89]|0987654321|charlie@example.com|180.3 |75.1  |[43 68 61 72 6C 69 65 42 69 6E 61 72 79]|3  |\n",
      "+-------+---+--------+-------------+----------+-------------------+-------------+------------+----------+-------------------+------+------+----------------------------------------+---+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import *\n",
    "\n",
    "# Initialize Spark session\n",
    "spark = SparkSession.builder.appName(\"PySparkExamples\").getOrCreate()\n",
    "\n",
    "# Define Schema\n",
    "from pyspark.sql.types import StructType, StructField, IntegerType, StringType, ArrayType, BinaryType, FloatType\n",
    "\n",
    "schema = StructType([\n",
    "    StructField(\"Name\", StringType(), True),\n",
    "    StructField(\"Age\", IntegerType(), True),\n",
    "    StructField(\"Salary\", FloatType(), True),\n",
    "    StructField(\"City\", StringType(), True),\n",
    "    StructField(\"JoinDate\", StringType(), True),  # Date stored as string\n",
    "    StructField(\"Skills1\", ArrayType(StringType()), True),\n",
    "    StructField(\"Skills2\", ArrayType(StringType()), True),\n",
    "    StructField(\"Scores\", ArrayType(IntegerType()), True),\n",
    "    StructField(\"Phone\", StringType(), True),\n",
    "    StructField(\"Email\", StringType(), True),\n",
    "    StructField(\"Height\", FloatType(), True),\n",
    "    StructField(\"Weight\", FloatType(), True),\n",
    "    StructField(\"BinaryData\", BinaryType(), True),\n",
    "    StructField(\"ID\", IntegerType(), True)\n",
    "])\n",
    "\n",
    "# Sample Data\n",
    "data = [\n",
    "    (\"Alice\", 30, 60000.50, \"New York\", \"2020-01-15\", [\"Python\", \"SQL\"], [\"SQL\", \"Java\"], [90, 85, 88], None, \"alice@example.com\", 165.2, 60.5, b\"AliceBinary\", 1),\n",
    "    (\"Bob\", 25, 50000.75, \"London\", \"2019-07-21\", [\"Java\", \"C++\"], [\"Python\", \"C++\"], [78, 80, 82], \"1234567890\", None, 170.4, 68.2, b\"BobBinary\", 2),\n",
    "    (\"Charlie\", 35, 75000.00, \"San Francisco\", \"2018-05-10\", [\"JavaScript\", \"Scala\"], [\"Scala\", \"Rust\"], [95, 92, 89], \"0987654321\", \"charlie@example.com\", 180.3, 75.1, b\"CharlieBinary\", 3)\n",
    "]\n",
    "\n",
    "# Create DataFrame\n",
    "df = spark.createDataFrame(data, schema=schema)\n",
    "df.show(truncate=False)\n",
    "\n",
    "# # 🔹 Numeric Functions\n",
    "# df.select(\"Name\", \"Age\", abs(df[\"Age\"]).alias(\"AbsoluteAge\")).show()\n",
    "# df.select(\"Name\", \"Salary\", ceil(df[\"Salary\"]).alias(\"CeilingSalary\")).show()\n",
    "# df.select(\"Name\", \"Age\", cbrt(df[\"Age\"]).alias(\"CubeRootAge\")).show()\n",
    "# df.select(\"Name\", \"Age\", acos(df[\"Age\"]/100).alias(\"AcosAge\")).show()\n",
    "# df.select(\"Name\", \"Age\", asin(df[\"Age\"]/100).alias(\"AsinAge\")).show()\n",
    "# df.select(\"Name\", \"Age\", atan(df[\"Age\"]/100).alias(\"AtanAge\")).show()\n",
    "# df.select(\"Name\", \"Height\", \"Weight\", atan2(df[\"Height\"], df[\"Weight\"]).alias(\"Atan2HeightWeight\")).show()\n",
    "\n",
    "# # 🔹 Array Functions\n",
    "# df.select(\"Name\", \"Skills1\", \"Skills2\", array_intersect(df[\"Skills1\"], df[\"Skills2\"]).alias(\"CommonSkills\")).show()\n",
    "# df.select(\"Name\", \"Scores\", array_max(df[\"Scores\"]).alias(\"MaxScore\")).show()\n",
    "# df.select(\"Name\", \"Scores\", array_min(df[\"Scores\"]).alias(\"MinScore\")).show()\n",
    "# df.select(\"Name\", \"Skills1\", array_join(df[\"Skills1\"], \", \").alias(\"SkillsAsString\")).show()\n",
    "# df.select(\"Name\", \"Skills1\", array_repeat(df[\"Skills1\"], 2).alias(\"RepeatedSkills\")).show()\n",
    "# df.select(\"Name\", \"Skills1\", array_sort(df[\"Skills1\"]).alias(\"SortedSkills\")).show()\n",
    "# df.select(\"Name\", \"Skills1\", \"Skills2\", arrays_zip(df[\"Skills1\"], df[\"Skills2\"]).alias(\"ZippedSkills\")).show()\n",
    "\n",
    "# # 🔹 String Functions\n",
    "# df.select(\"Name\", ascii(df[\"Name\"]).alias(\"ASCII_FirstChar\")).show()\n",
    "# df.select(\"Name\", base64(df[\"BinaryData\"]).alias(\"Base64Encoded\")).show()\n",
    "\n",
    "# # 🔹 Bitwise & Binary Functions\n",
    "# df.select(\"Name\", \"ID\", bin(df[\"ID\"]).alias(\"BinaryRepresentation\")).show()\n",
    "# df.select(\"Name\", \"ID\", bitwise_not(df[\"ID\"]).alias(\"BitwiseNotID\")).show()\n",
    "\n",
    "# # 🔹 Date Functions\n",
    "# df.select(\"Name\", \"JoinDate\", add_months(df[\"JoinDate\"], 3).alias(\"DateAfter3Months\")).show()\n",
    "\n",
    "# # 🔹 Aggregation Functions\n",
    "# df.groupBy(\"City\").agg(avg(df[\"Salary\"]).alias(\"AverageSalary\")).show()\n",
    "# df.groupBy(\"City\").agg(collect_list(df[\"Name\"]).alias(\"PeopleInCity\")).show()\n",
    "# df.groupBy(\"City\").agg(collect_set(df[\"Name\"]).alias(\"UniquePeopleInCity\")).show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+---+--------+-------------+----------+-------------------+-------------+------------+----------+-------------------+------+------+--------------------+---+\n",
      "|   Name|Age|  Salary|         City|  JoinDate|            Skills1|      Skills2|      Scores|     Phone|              Email|Height|Weight|          BinaryData| ID|\n",
      "+-------+---+--------+-------------+----------+-------------------+-------------+------------+----------+-------------------+------+------+--------------------+---+\n",
      "|  Alice| 30| 60000.5|     New York|2020-01-15|      [Python, SQL]|  [SQL, Java]|[90, 85, 88]|      null|  alice@example.com| 165.2|  60.5|[41 6C 69 63 65 4...|  1|\n",
      "|    Bob| 25|50000.75|       London|2019-07-21|        [Java, C++]|[Python, C++]|[78, 80, 82]|1234567890|               null| 170.4|  68.2|[42 6F 62 42 69 6...|  2|\n",
      "|Charlie| 35| 75000.0|San Francisco|2018-05-10|[JavaScript, Scala]|[Scala, Rust]|[95, 92, 89]|0987654321|charlie@example.com| 180.3|  75.1|[43 68 61 72 6C 6...|  3|\n",
      "+-------+---+--------+-------------+----------+-------------------+-------------+------------+----------+-------------------+------+------+--------------------+---+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.select(\"Name\", \"JoinDate\", add_months(df[\"2012-12-12\"], 3).alias(\"DateAfter3Months\")).show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Jira Ticket"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "code "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Project: Employee Data Processing with PySpark**\n",
    "\n",
    "This project will help you prepare for your PySpark interview by applying common functions, transformations, and data processing techniques. The dataset will be sourced from an open-source repository, and the project will cover tasks like data cleaning, transformations, aggregations, and saving the processed data to AWS S3.\n",
    "\n",
    "---\n",
    "\n",
    "### **Project Structure**\n",
    "\n",
    "1. **Objective:** Load, clean, and transform employee data.\n",
    "2. **Dataset:** Use a publicly available employee dataset (e.g., from [Kaggle Employee Data](https://www.kaggle.com/datasets)).\n",
    "3. **Tools:** PySpark for data processing, AWS S3 for storage.\n",
    "\n",
    "---\n",
    "\n",
    "### **Step 1: Setup PySpark and Download Dataset**\n",
    "Make sure you have the necessary libraries installed.\n",
    "\n",
    "#### Install PySpark:\n",
    "```bash\n",
    "pip install pyspark\n",
    "```\n",
    "\n",
    "Download the dataset from Kaggle or any open-source platform. Here, let's assume you're using the \"Employee.csv\" dataset with the following columns:\n",
    "- `Name`\n",
    "- `Age`\n",
    "- `JoinDate`\n",
    "- `Department`\n",
    "- `Skills`\n",
    "- `Salary`\n",
    "\n",
    "---\n",
    "\n",
    "### **Step 2: Initialize PySpark Session**\n",
    "Create a Spark session to load and process data.\n",
    "\n",
    "```python\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import *\n",
    "\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"EmployeeDataProcessing\") \\\n",
    "    .getOrCreate()\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "### **Step 3: Load the Dataset**\n",
    "Use PySpark's `csv` reader to load the dataset.\n",
    "\n",
    "```python\n",
    "# Load the CSV dataset\n",
    "df = spark.read.option(\"header\", \"true\").csv(\"employee_data.csv\", inferSchema=True)\n",
    "\n",
    "# Show the top 5 rows of the dataset\n",
    "df.show(5)\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "### **Step 4: Data Cleaning and Transformation**\n",
    "\n",
    "#### 4.1. **Handling Missing Values**\n",
    "Replace missing values for `Department` with \"Unknown\" and `Salary` with 0.\n",
    "\n",
    "```python\n",
    "df = df.fillna({\"Department\": \"Unknown\", \"Salary\": 0})\n",
    "```\n",
    "\n",
    "#### 4.2. **Convert Data Types**\n",
    "Ensure that columns have the correct data types.\n",
    "\n",
    "```python\n",
    "df = df.withColumn(\"Age\", df[\"Age\"].cast(\"int\"))\n",
    "df = df.withColumn(\"Salary\", df[\"Salary\"].cast(\"double\"))\n",
    "df = df.withColumn(\"JoinDate\", to_date(df[\"JoinDate\"], \"yyyy-MM-dd\"))\n",
    "```\n",
    "\n",
    "#### 4.3. **Feature Engineering**\n",
    "Use PySpark functions to add new columns and perform calculations.\n",
    "\n",
    "- Add 3 months to `JoinDate`\n",
    "- Calculate new `Salary` after a 10% increment\n",
    "- Extract the first skill from the `Skills` array\n",
    "\n",
    "```python\n",
    "df = df.withColumn(\"DateAfter3Months\", add_months(df[\"JoinDate\"], 3))\n",
    "df = df.withColumn(\"NewSalary\", df[\"Salary\"] * 1.1)\n",
    "df = df.withColumn(\"FirstSkill\", df[\"Skills\"].getItem(0))\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "### **Step 5: Data Analysis with PySpark Functions**\n",
    "\n",
    "#### 5.1. **Find the Average Salary per Department**\n",
    "```python\n",
    "df.groupBy(\"Department\").agg(avg(\"Salary\").alias(\"AvgSalary\")).show()\n",
    "```\n",
    "\n",
    "#### 5.2. **Find Employees Who Know \"Python\"**\n",
    "```python\n",
    "df.filter(array_contains(df[\"Skills\"], \"Python\")).show()\n",
    "```\n",
    "\n",
    "#### 5.3. **Find the Youngest and Oldest Employees**\n",
    "```python\n",
    "df.select(min(\"Age\").alias(\"Youngest\"), max(\"Age\").alias(\"Oldest\")).show()\n",
    "```\n",
    "\n",
    "#### 5.4. **List Unique Departments**\n",
    "```python\n",
    "df.select(collect_set(\"Department\")).show()\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "### **Step 6: Save Processed Data to AWS S3 using Multipart Upload**\n",
    "\n",
    "To use multipart uploads, you can store the processed data in S3 after partitioning it into smaller files.\n",
    "\n",
    "```python\n",
    "output_path = \"s3://your-bucket-name/processed_employees/\"\n",
    "df.write.mode(\"overwrite\").parquet(output_path)\n",
    "```\n",
    "\n",
    "#### Save to S3\n",
    "Before you run this, make sure you've set up your AWS credentials using either environment variables or an IAM role.\n",
    "\n",
    "### **Step 7: Combine Multiple Files into One (Optional)**\n",
    "If the data has been split across multiple files and you want to combine them, you can use the following:\n",
    "\n",
    "```python\n",
    "# Read two files and merge\n",
    "df1 = spark.read.parquet(\"s3://your-bucket-name/file1.parquet\")\n",
    "df2 = spark.read.parquet(\"s3://your-bucket-name/file2.parquet\")\n",
    "\n",
    "# Merge them into a single DataFrame\n",
    "merged_df = df1.union(df2)\n",
    "\n",
    "# Write the merged data to S3\n",
    "merged_df.write.mode(\"overwrite\").parquet(\"s3://your-bucket-name/merged_employees.parquet\")\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "### **Step 8: Example of Interview Questions Based on Functions Used**\n",
    "\n",
    "| Function | What It Does | Example | Frequency (1-5) |\n",
    "|----------|--------------|---------|-----------------|\n",
    "| `abs()` | Returns the absolute value of a column. | `df.select(\"Age\", abs(df[\"Age\"])).show()` | 3 |\n",
    "| `add_months()` | Adds months to a date. | `df.select(add_months(df[\"JoinDate\"], 3).alias(\"DateAfter3Months\")).show()` | 4 |\n",
    "| `array_contains()` | Checks if an array contains a value. | `df.filter(array_contains(df[\"Skills\"], \"Python\")).show()` | 4 |\n",
    "| `avg()` | Calculates the average of a numeric column. | `df.groupBy(\"Department\").agg(avg(\"Salary\").alias(\"AvgSalary\")).show()` | 5 |\n",
    "| `collect_list()` | Collects the values of a column into a list. | `df.groupBy(\"Department\").agg(collect_list(\"Name\")).show()` | 3 |\n",
    "| `min()`, `max()` | Finds the minimum or maximum value. | `df.select(min(\"Age\"), max(\"Age\")).show()` | 4 |\n",
    "| `coalesce()` | Returns the first non-null value. | `df.select(coalesce(df[\"Phone\"], df[\"Email\"]).alias(\"PreferredContact\")).show()` | 3 |\n",
    "\n",
    "---\n",
    "\n",
    "### **Conclusion**\n",
    "This project demonstrates how to:\n",
    "- Use **PySpark SQL functions** for data cleaning, transformation, and analysis.\n",
    "- Perform **feature engineering** using built-in functions like `add_months`, `abs`, `array_contains`.\n",
    "- **Aggregate** data using functions like `avg`, `min`, `max`.\n",
    "- Save the data to **AWS S3** using multipart upload and partitioning.\n",
    "\n",
    "### **Next Steps:**\n",
    "- Implement additional features like error handling and logging.\n",
    "- Set up a job scheduler (e.g., Apache Airflow) for automated execution.\n",
    "- Extend the project to handle larger datasets and optimize the performance.\n",
    "\n",
    "This project can be used as a base for interview preparation and hands-on experience. Let me know if you need more help or specific instructions!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Unittest"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For your data pipeline project, I will provide a sample **unit test** code using **PySpark**'s `unittest` framework and `pytest` for testing. This assumes the pipeline processes some data transformations (such as filtering, aggregating, or joining data).\n",
    "\n",
    "### Step-by-Step Unit Test Setup:\n",
    "- We will simulate a **data pipeline** that processes a DataFrame.\n",
    "- I'll create a few example transformations, then create unit tests to validate that the expected results are returned.\n",
    "\n",
    "Let's assume a simple pipeline:\n",
    "1. **Data loading**: A sample CSV file or data is loaded into a DataFrame.\n",
    "2. **Data Transformation**: A few transformations are applied (filter, aggregation, etc.).\n",
    "3. **Unit Test**: Validate that the transformation works correctly using assertions.\n",
    "\n",
    "Here is how we can create the test code:\n",
    "\n",
    "### Example Data Pipeline Code\n",
    "Let's assume the pipeline contains the following transformations:\n",
    "\n",
    "```python\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col, avg\n",
    "\n",
    "class DataPipeline:\n",
    "    def __init__(self, spark_session: SparkSession):\n",
    "        self.spark = spark_session\n",
    "\n",
    "    def load_data(self, data):\n",
    "        \"\"\" Load data into DataFrame \"\"\"\n",
    "        return self.spark.createDataFrame(data)\n",
    "\n",
    "    def filter_data(self, df):\n",
    "        \"\"\" Filter out rows where 'age' is less than 30 \"\"\"\n",
    "        return df.filter(col(\"age\") >= 30)\n",
    "\n",
    "    def calculate_avg_salary(self, df):\n",
    "        \"\"\" Calculate the average salary \"\"\"\n",
    "        return df.agg(avg(\"salary\").alias(\"avg_salary\")).collect()[0][\"avg_salary\"]\n",
    "\n",
    "    def process_pipeline(self, data):\n",
    "        \"\"\" Full pipeline: load, filter and calculate average salary \"\"\"\n",
    "        df = self.load_data(data)\n",
    "        df = self.filter_data(df)\n",
    "        return self.calculate_avg_salary(df)\n",
    "```\n",
    "\n",
    "### Unit Test Code for Data Pipeline\n",
    "\n",
    "```python\n",
    "import unittest\n",
    "from pyspark.sql import SparkSession\n",
    "from data_pipeline import DataPipeline  # Assuming the pipeline code is in a file named `data_pipeline.py`\n",
    "\n",
    "class DataPipelineTests(unittest.TestCase):\n",
    "    @classmethod\n",
    "    def setUpClass(cls):\n",
    "        \"\"\" Set up the Spark session before any tests \"\"\"\n",
    "        cls.spark = SparkSession.builder.master(\"local[1]\").appName(\"DataPipelineTest\").getOrCreate()\n",
    "        cls.pipeline = DataPipeline(cls.spark)\n",
    "\n",
    "    def test_load_data(self):\n",
    "        \"\"\" Test data loading into DataFrame \"\"\"\n",
    "        test_data = [(\"John\", 35, 1000), (\"Anna\", 28, 1500)]\n",
    "        df = self.pipeline.load_data(test_data)\n",
    "\n",
    "        # Check if DataFrame is loaded correctly\n",
    "        self.assertEqual(df.count(), 2)\n",
    "\n",
    "    def test_filter_data(self):\n",
    "        \"\"\" Test filtering data where 'age' >= 30 \"\"\"\n",
    "        test_data = [(\"John\", 35, 1000), (\"Anna\", 28, 1500)]\n",
    "        df = self.pipeline.load_data(test_data)\n",
    "        filtered_df = self.pipeline.filter_data(df)\n",
    "\n",
    "        # Verify that 'Anna' is filtered out (she's under 30)\n",
    "        self.assertEqual(filtered_df.count(), 1)\n",
    "        self.assertEqual(filtered_df.collect()[0][\"name\"], \"John\")\n",
    "\n",
    "    def test_calculate_avg_salary(self):\n",
    "        \"\"\" Test calculating the average salary \"\"\"\n",
    "        test_data = [(\"John\", 35, 1000), (\"Anna\", 28, 1500)]\n",
    "        df = self.pipeline.load_data(test_data)\n",
    "        filtered_df = self.pipeline.filter_data(df)\n",
    "        avg_salary = self.pipeline.calculate_avg_salary(filtered_df)\n",
    "\n",
    "        # The only record left is John, so the average salary is 1000\n",
    "        self.assertEqual(avg_salary, 1000)\n",
    "\n",
    "    def test_process_pipeline(self):\n",
    "        \"\"\" Test the entire data pipeline process \"\"\"\n",
    "        test_data = [(\"John\", 35, 1000), (\"Anna\", 28, 1500)]\n",
    "        avg_salary = self.pipeline.process_pipeline(test_data)\n",
    "\n",
    "        # The average salary after filtering 'Anna' should be 1000\n",
    "        self.assertEqual(avg_salary, 1000)\n",
    "\n",
    "    @classmethod\n",
    "    def tearDownClass(cls):\n",
    "        \"\"\" Stop the Spark session after tests \"\"\"\n",
    "        cls.spark.stop()\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    unittest.main()\n",
    "```\n",
    "\n",
    "### Explanation:\n",
    "- **setUpClass()**: Initializes the `SparkSession` before running any tests. This method is run once for the entire test class.\n",
    "- **test_load_data()**: Tests if the data is loaded correctly into a DataFrame.\n",
    "- **test_filter_data()**: Verifies that the `filter_data()` method correctly filters out rows where `age` is less than 30.\n",
    "- **test_calculate_avg_salary()**: Tests the logic of calculating the average salary after the filtering.\n",
    "- **test_process_pipeline()**: Validates the entire pipeline by testing `process_pipeline()`.\n",
    "- **tearDownClass()**: Stops the Spark session after all tests have been executed.\n",
    "\n",
    "### Running the Tests:\n",
    "You can run the tests using the following command in your terminal:\n",
    "\n",
    "```bash\n",
    "pytest test_data_pipeline.py\n",
    "```\n",
    "\n",
    "This will run the unit tests and give you a summary of the test results.\n",
    "\n",
    "### Key Considerations:\n",
    "- Make sure to have a **test-friendly dataset** (mock data) so the tests remain isolated and deterministic.\n",
    "- You can use additional **assertions** like `assertEqual()`, `assertGreater()`, or `assertTrue()` to validate expected behaviors.\n",
    "- The **SparkSession** is only created once for the entire test class to save time.\n",
    "\n",
    "This is a simple example of testing your data pipeline, but you can expand on it to include more complex logic, error handling, and edge cases."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
