{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Employee Data Processing with PySpark\n",
    "This notebook demonstrates an end-to-end PySpark data processing pipeline using an employee dataset. It covers:\n",
    "\n",
    "- Data ingestion (using sample data)\n",
    "- Data cleaning & transformation\n",
    "- Feature engineering\n",
    "- Aggregation & analysis\n",
    "- Applying a wide variety of PySpark SQL functions (statistical, date functions, window functions, string operations, etc.)\n",
    "\n",
    "\n",
    "Dataset Source: You can use the Human Resources Data Set from Kaggle or any other similar open-source employee dataset. In this example, we simulate sample data for demonstration."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Setup Spark Session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import when, col, collect_list, collect_set, lit, to_date, to_timestamp, split,\\\n",
    "    trim, min, expr, add_months, format_number, window, cume_dist, current_date, current_timestamp, dense_rank, \\\n",
    "    rank, row_number, month, dayofmonth, dayofweek, dayofyear, month, dayofmonth, dayofweek, dayofyear, \\\n",
    "    add_months,month,months_between, decode\n",
    "from pyspark.sql.types import StructType, StructField, StringType, IntegerType, DoubleType, DateType, StructField, StructType, BinaryType\n",
    "from pyspark.sql.window import Window\n",
    "\n",
    "\n",
    "from awsglue.context import GlueContext\n",
    "from pyspark.context import SparkContext\n",
    "from awsglue.dynamicframe import DynamicFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/glue_user/spark/python/pyspark/sql/context.py:112: FutureWarning: Deprecated in 3.0.0. Use SparkSession.builder.getOrCreate() instead.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "spark = SparkSession.builder.appName(\"HRSparkFunctionsSession\").config(\"spark.executor.memory\", \"2g\").config(\"spark.driver.memory\", \"2g\").config(\"spark.sql.shuffle.partitions\", \"4\").config(\"spark.ui.port\", \"4040\").getOrCreate()\n",
    "\n",
    "\n",
    "glueContext = GlueContext(spark.sparkContext)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Create Sample DataFrame\n",
    "\n",
    "For demonstration, we create a sample DataFrame with columns similar to those in an HR dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/glue_user/spark/python/pyspark/sql/dataframe.py:127: UserWarning: DataFrame constructor is internal. Do not directly use it.\n",
      "  warnings.warn(\"DataFrame constructor is internal. Do not directly use it.\")\n"
     ]
    }
   ],
   "source": [
    "# Employee_Name,EmpID,MarriedID,MaritalStatusID,GenderID,EmpStatusID,DeptID,PerfScoreID,FromDiversityJobFairID,Salary,Termd,PositionID,Position,State,Zip,DOB,Sex,MaritalDesc,CitizenDesc,HispanicLatino,RaceDesc,DateofHire,DateofTermination,TermReason,EmploymentStatus,Department,ManagerName,ManagerID,RecruitmentSource,PerformanceScore,EngagementSurvey,EmpSatisfaction,SpecialProjectsCount,LastPerformanceReview_Date,DaysLateLast30,Absences\n",
    "data_skills = \"python,java, c, devops\"\n",
    "schema = StructType([\n",
    "    StructField(\"Employee_Name\", StringType(), True),\n",
    "    StructField(\"EmpID\", IntegerType(), True),\n",
    "    StructField(\"MarriedID\", IntegerType(), True),\n",
    "    StructField(\"MaritalStatusID\", IntegerType(), True),\n",
    "    StructField(\"GenderID\", IntegerType(), True),\n",
    "    StructField(\"EmpStatusID\", IntegerType(), True),\n",
    "    StructField(\"DeptID\", IntegerType(), True),\n",
    "    StructField(\"PerfScoreID\", IntegerType(), True),\n",
    "    StructField(\"FromDiversityJobFairID\", IntegerType(), True),\n",
    "    StructField(\"Salary\", DoubleType(), True),\n",
    "    StructField(\"Termd\", StringType(), True),  # Could also be BooleanType() if represented as True/False\n",
    "    StructField(\"PositionID\", IntegerType(), True),\n",
    "    StructField(\"Position\", StringType(), True),\n",
    "    StructField(\"State\", StringType(), True),\n",
    "    StructField(\"Zip\", StringType(), True),  # Using StringType for ZIP codes to preserve any leading zeros\n",
    "    StructField(\"DOB\", StringType(), True),\n",
    "    StructField(\"Sex\", StringType(), True),\n",
    "    StructField(\"MaritalDesc\", StringType(), True),\n",
    "    StructField(\"CitizenDesc\", StringType(), True),\n",
    "    StructField(\"HispanicLatino\", StringType(), True),  # Change to BooleanType() if appropriate\n",
    "    StructField(\"RaceDesc\", StringType(), True),\n",
    "    StructField(\"DateofHire\", StringType(), True),\n",
    "    StructField(\"DateofTermination\", StringType(), True),\n",
    "    StructField(\"TermReason\", StringType(), True),\n",
    "    StructField(\"EmploymentStatus\", StringType(), True),\n",
    "    StructField(\"Department\", StringType(), True),\n",
    "    StructField(\"ManagerName\", StringType(), True),\n",
    "    StructField(\"ManagerID\", IntegerType(), True),\n",
    "    StructField(\"RecruitmentSource\", StringType(), True),\n",
    "    StructField(\"PerformanceScore\", StringType(), True),  # Or IntegerType() if numerical\n",
    "    StructField(\"EngagementSurvey\", DoubleType(), True),\n",
    "    StructField(\"EmpSatisfaction\", DoubleType(), True),\n",
    "    StructField(\"SpecialProjectsCount\", IntegerType(), True),\n",
    "    StructField(\"LastPerformanceReview_Date\", StringType(), True),\n",
    "    StructField(\"DaysLateLast30\", IntegerType(), True),\n",
    "    StructField(\"Absences\", IntegerType(), True)\n",
    "])\n",
    "\n",
    "\n",
    "local_df = spark.read.format('csv').option(\"header\", \"true\").option(\"mode\",\"PERMISSIVE\").option(\"columnNameOfCorruptRecord\", \"_corrupt_record\").schema(schema).load('/mount_folder/alpha/NYC_Taxi_Data_Pipeline_git/Practice/pyspark_functions/Employeee/HRDataset_v14.csv')\n",
    "dynamic_frame = DynamicFrame.fromDF(local_df, glueContext, \"dynamic_frame\")\n",
    "# dynamic_frame.show()\n",
    "df = dynamic_frame.toDF()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------+-----+---------+---------------+--------+-----------+------+-----------+----------------------+-------+-----+----------+--------------------+-----+-----+--------+---+-----------+-----------+--------------+--------+----------+-----------------+-----------------+----------------+-----------------+--------------+---------+-----------------+----------------+----------------+---------------+--------------------+--------------------------+--------------+--------+\n",
      "|      Employee_Name|EmpID|MarriedID|MaritalStatusID|GenderID|EmpStatusID|DeptID|PerfScoreID|FromDiversityJobFairID| Salary|Termd|PositionID|            Position|State|  Zip|     DOB|Sex|MaritalDesc|CitizenDesc|HispanicLatino|RaceDesc|DateofHire|DateofTermination|       TermReason|EmploymentStatus|       Department|   ManagerName|ManagerID|RecruitmentSource|PerformanceScore|EngagementSurvey|EmpSatisfaction|SpecialProjectsCount|LastPerformanceReview_Date|DaysLateLast30|Absences|\n",
      "+-------------------+-----+---------+---------------+--------+-----------+------+-----------+----------------------+-------+-----+----------+--------------------+-----+-----+--------+---+-----------+-----------+--------------+--------+----------+-----------------+-----------------+----------------+-----------------+--------------+---------+-----------------+----------------+----------------+---------------+--------------------+--------------------------+--------------+--------+\n",
      "|Adinolfi, Wilson  K|10026|        0|              0|       1|          1|     5|          4|                     0|62506.0|    0|        19|Production Techni...|   MA|01960|07/10/83| M |     Single| US Citizen|            No|   White|  7/5/2011|             null|N/A-StillEmployed|          Active|Production       |Michael Albert|       22|         LinkedIn|         Exceeds|             4.6|            5.0|                   0|                 1/17/2019|             0|       1|\n",
      "+-------------------+-----+---------+---------------+--------+-----------+------+-----------+----------------------+-------+-----+----------+--------------------+-----+-----+--------+---+-----------+-----------+--------------+--------+----------+-----------------+-----------------+----------------+-----------------+--------------+---------+-----------------+----------------+----------------+---------------+--------------------+--------------------------+--------------+--------+\n",
      "only showing top 1 row\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.show(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Data Cleaning & Transformation\n",
    "\n",
    "\n",
    "- Convert date and time columns to proper data types (using to_date(), to_timestamp(), etc.).\n",
    "- Handle null values, trim spaces, and drop duplicates as necessary.\n",
    "- Standardize string fields (e.g., using lower(), upper(), lpad())."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_skills = data_skills.replace(\" \",'')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from pyspark.sql.functions import udf\n",
    "# from pyspark.sql.types import ArrayType, StringType\n",
    "\n",
    "# # Define a UDF to split and trim the Skills string\n",
    "# def process_skills(skills_str):\n",
    "#     return [skill.strip() for skill in skills_str.split(\",\")]\n",
    "\n",
    "# process_skills_udf = udf(process_skills, (StringType()))\n",
    "\n",
    "# # Add the Skills column using the UDF\n",
    "# df = df.withColumn(\"Skills\", process_skills_udf(lit(data_skills)))\n",
    "\n",
    "# # Show the DataFrame\n",
    "# df.select(\"Skills\").show(truncate=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [],
   "source": [
    "min_dateofhire = df.select(min(df[\"DateofHire\"])).collect()[0][0]\n",
    "\n",
    "df_temp =df.withColumn(\"to_date_DateofHire\", to_date(trim('DateofHire') , 'M/d/yyyy')).withColumn(\"to_date_LastPerformanceReviewDate\", to_date(trim('LastPerformanceReview_Date') , 'M/d/yyyy')).withColumn(\"to_date_DateofTermination\", to_date(trim('DateofTermination') , 'M/d/yyyy')).withColumn(\"Skills\", lit(data_skills))\n",
    "\n",
    "df  = df_temp.fillna({\"to_date_DateofHire\":min_dateofhire})      \n",
    "                \n",
    "\n",
    "df = df.drop(\"DateofHire\",\"LastPerformanceReview_Date\", \"DateofTermination\")\n",
    "\n",
    "df = df.withColumnRenamed(\"to_date_DateofHire\", \"DateofHire\").withColumnRenamed(\"to_date_LastPerformanceReviewDate\",\"LastPerformanceReview_Date\").withColumnRenamed(\"to_date_DateofTermination\",\"DateofTermination\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Feature Engineering\n",
    "\n",
    "Create new features using various PySpark SQL functions.\n",
    "\n",
    "- Compute new features like bonus amount, total compensation, and probation end dates (using add_months(), arithmetic operations, etc.).\n",
    "- Split string columns into arrays (using split()), and process array data (using explode())."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.withColumn(\"Department1\", trim(df['Department']))\n",
    "df = df.withColumnRenamed(\"Department1\",\"Department\")\n",
    "df = df.drop(\"Department1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[23]\n"
     ]
    }
   ],
   "source": [
    "df_cols = df.columns\n",
    "# get index of the duplicate columns\n",
    "duplicate_col_index = list(set([df_cols.index(c) for c in df_cols if df_cols.count(c) == 2]))\n",
    "print(duplicate_col_index)\n",
    "# rename by adding suffix '_duplicated'\n",
    "for i in duplicate_col_index:\n",
    "    df_cols[i] = df_cols[i] + '_duplicated'\n",
    "\n",
    "# rename the column in DF\n",
    "df = df.toDF(*df_cols)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.drop(\"Department_duplicated\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(department='Production'),\n",
       " Row(department='Sales'),\n",
       " Row(department='IT/IS'),\n",
       " Row(department='Software Engineering'),\n",
       " Row(department='Admin Offices'),\n",
       " Row(department='Executive Office')]"
      ]
     },
     "execution_count": 137,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.select(df['department']).distinct().collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [],
   "source": [
    "#adding bonus department wise\n",
    "df = df.withColumn(\"Bonus\",when(df[\"department\"] == \"Production\", 4.5).when(df[\"department\"] == \"Sales\", 4).when(df[\"department\"] == \"IT/IS\", 4.5).when(df[\"department\"] == \"Software Engineering\", 3).when(df[\"department\"] == \"Admin Offices\", 3).when(df[\"department\"] == \"Executive Office\", 3)).withColumn(\"Probation_end_date\", add_months(df[\"DateofHire\"], 3)).withColumn(\"formatedSalary\", format_number(df['salary'], 2)).withColumn(\"Bonus_Amount\", col(\"Salary\") * col(\"Bonus\") / 100).withColumn(\"Total_Compensation\", col(\"Salary\") + col(\"Bonus_Amount\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Aggregation & Analysis:\n",
    "\n",
    "- Group data by relevant keys (e.g., Team) and compute aggregations (using avg(), sum(), collect_list(), collect_set()).\n",
    "- Apply window functions for ranking and lag/lead operations (using dense_rank(), lag(), lead(), cume_dist())."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5. Applying Additional PySpark SQL Functions\n",
    "\n",
    "Below we demonstrate each additional function from your list.\n",
    "\n",
    "- Demonstrate usage of statistical, date, and string functions such as exp(), floor(), format_number(), from_unixtime(), hour(), second(), pmod(), rand(), round(), substring(), etc.\n",
    "- Ensure each of the functions listed in the provided table is used at least once in the pipeline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 6. Aggregation & Analysis Example\n",
    "\n",
    "\n",
    "We combine aggregated data back into the original DataFrame (join on \"Team\" & optionally \"Gender\"). Here’s an example join (using only \"Team\" for simplicity):\n",
    "\n",
    "\n",
    "- Optimize Spark configurations (e.g., spark.executor.memory, spark.sql.shuffle.partitions).\n",
    "- Cache intermediate DataFrames when necessary.\n",
    "- Use broadcast joins, partitioning, and column pruning where applicable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [],
   "source": [
    "windowSpec = Window.partitionBy(\"Department\").orderBy(\"Salary\")\n",
    "df = df.withColumn(\"CumeDist\", cume_dist().over(windowSpec))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.withColumn(\"Current_date\", current_date()).withColumn(\"Current_timestamp\", current_timestamp())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3. dense_rank() - Rank employees by Salary (no gaps)\n",
    "# department_wise\n",
    "window_dept_salary = Window.partitionBy(\"Department\").orderBy(\"salary\")\n",
    "df = df.withColumn(\"sal_rank_by_department\", dense_rank().over(window_dept_salary))\n",
    "\n",
    "sal_rank_emp = Window.orderBy('Salary')\n",
    "df = df.withColumn(\"sal_rank_empDense\", dense_rank().over(sal_rank_emp))\n",
    "df = df.withColumn(\"sal_rank_empRow\", row_number().over(sal_rank_emp))\n",
    "df = df.withColumn(\"sal_rank_empRank\", rank().over(sal_rank_emp))\n",
    "\n",
    "\n",
    "\n",
    "# # 5. dayofmonth(), dayofweek(), dayofyear() from Start Date\n",
    "df = df.withColumn(\"month\", dayofmonth(df['DateofHire'])).withColumn(\"day\",dayofweek(df['DateofHire'])).withColumn(\"day\",dayofyear(df['DateofHire']))\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# 6. decode() - Decode BinaryData column (assume UTF-8)\n",
    "# adding columns\n",
    "binary_data = bytearray(b'hello')\n",
    "df = df.withColumn(\"BinaryData\", lit(binary_data).cast(BinaryType()))\n",
    "\n",
    "df = df.withColumn('decode_of_BinaryData', decode(df['BinaryData'], 'utf-8'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import exp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Column<'EXP(2)'>"
      ]
     },
     "execution_count": 152,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "exp(lit(2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Employee_Name',\n",
       " 'EmpID',\n",
       " 'MarriedID',\n",
       " 'MaritalStatusID',\n",
       " 'GenderID',\n",
       " 'EmpStatusID',\n",
       " 'DeptID',\n",
       " 'PerfScoreID',\n",
       " 'FromDiversityJobFairID',\n",
       " 'Salary',\n",
       " 'Termd',\n",
       " 'PositionID',\n",
       " 'Position',\n",
       " 'State',\n",
       " 'Zip',\n",
       " 'DOB',\n",
       " 'Sex',\n",
       " 'MaritalDesc',\n",
       " 'CitizenDesc',\n",
       " 'HispanicLatino',\n",
       " 'RaceDesc',\n",
       " 'TermReason',\n",
       " 'EmploymentStatus',\n",
       " 'ManagerName',\n",
       " 'ManagerID',\n",
       " 'RecruitmentSource',\n",
       " 'PerformanceScore',\n",
       " 'EngagementSurvey',\n",
       " 'EmpSatisfaction',\n",
       " 'SpecialProjectsCount',\n",
       " 'DaysLateLast30',\n",
       " 'Absences',\n",
       " 'DateofHire',\n",
       " 'LastPerformanceReview_Date',\n",
       " 'DateofTermination',\n",
       " 'Skills',\n",
       " 'Department',\n",
       " 'Bonus',\n",
       " 'Probation_end_date',\n",
       " 'formatedSalary',\n",
       " 'Bonus_Amount',\n",
       " 'Total_Compensation',\n",
       " 'CumeDist',\n",
       " 'Current_date',\n",
       " 'Current_timestamp',\n",
       " 'sal_rank_by_department',\n",
       " 'sal_rank_empDense',\n",
       " 'sal_rank_empRow',\n",
       " 'sal_rank_empRank',\n",
       " 'month',\n",
       " 'day']"
      ]
     },
     "execution_count": 114,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# # 7. exp() - Exponential of Salary (for demonstration)\n",
    "# df = df.withColumn(\"ExpSalary\", exp(col(\"Salary\")))\n",
    "\n",
    "# # 8. explode() - Explode the Skills array into individual rows\n",
    "# df_exploded = df.select(\"First Name\", explode(col(\"Skills\")).alias(\"Skill\"))\n",
    "\n",
    "# # 9. extract year (using year() function) from Start Date (already used above as HireYear)\n",
    "# df = df.withColumn(\"HireYear\", year(col(\"Start Date\")))\n",
    "\n",
    "# # 10. floor() - Floor of Salary\n",
    "# df = df.withColumn(\"FloorSalary\", floor(col(\"Salary\")))\n",
    "\n",
    "# # 11. format_number() - Already used above in FormattedSalary\n",
    "\n",
    "# # 12. from_unixtime() - Convert a Unix timestamp (simulate one)\n",
    "# df = df.withColumn(\"SimulatedUnix\", unix_timestamp(col(\"Start Date\"))) \\\n",
    "#        .withColumn(\"FormattedDate\", from_unixtime(col(\"SimulatedUnix\"), \"yyyy-MM-dd\"))\n",
    "\n",
    "# # 13. hour() - Extract hour from Last Login Time\n",
    "# df = df.withColumn(\"LoginHour\", hour(col(\"Last Login Time\")))\n",
    "\n",
    "# # 14. isnull() - Check if Phone column is null (simulate Phone column)\n",
    "# df = df.withColumn(\"Phone\", lit(None).cast(StringType()))\n",
    "# df = df.withColumn(\"PhoneIsNull\", isnull(col(\"Phone\")))\n",
    "\n",
    "# # 15. lag() and lead() - Previous and next Salary in ordering by Salary\n",
    "# window_spec_salary = Window.orderBy(\"Salary\")\n",
    "# df = df.withColumn(\"PrevSalary\", lag(col(\"Salary\"), 1).over(window_spec_salary)) \\\n",
    "#        .withColumn(\"NextSalary\", lead(col(\"Salary\"), 1).over(window_spec_salary))\n",
    "\n",
    "# # 16. length() - Length of the First Name\n",
    "# df = df.withColumn(\"NameLength\", length(col(\"First Name\")))\n",
    "\n",
    "# # 17. lower() - Already used above as LowerName\n",
    "\n",
    "# # 18. lpad() - Pad First Name to length 10 with \"*\"\n",
    "# df = df.withColumn(\"PaddedName\", lpad(col(\"First Name\"), 10, \"*\"))\n",
    "\n",
    "# # 19. max() and min() - Maximum and Minimum Salary (as aggregation example)\n",
    "# max_salary = df.agg(max(\"Salary\").alias(\"MaxSalary\")).collect()[0][\"MaxSalary\"]\n",
    "# min_salary = df.agg(min(\"Salary\").alias(\"MinSalary\")).collect()[0][\"MinSalary\"]\n",
    "\n",
    "# # 20. month() - Extract month from Start Date\n",
    "# df = df.withColumn(\"StartMonth\", month(col(\"Start Date\")))\n",
    "\n",
    "# # 21. nvl() - Replace null Phone with \"N/A\"\n",
    "# df = df.withColumn(\"PhoneNumber\", nvl(col(\"Phone\"), \"N/A\"))\n",
    "\n",
    "# # 22. pmod() - Salary modulo 10\n",
    "# df = df.withColumn(\"SalaryMod10\", pmod(col(\"Salary\"), 10))\n",
    "\n",
    "# # 23. rand() - Random number for each row (seed 100)\n",
    "# df = df.withColumn(\"RandomValue\", rand(100))\n",
    "\n",
    "# # 24. round() - Round Salary to nearest integer\n",
    "# df = df.withColumn(\"RoundedSalary\", round(col(\"Salary\"), 0))\n",
    "\n",
    "# # 25. second() - Extract seconds from Last Login Time\n",
    "# df = df.withColumn(\"LoginSecond\", second(col(\"Last Login Time\")))\n",
    "\n",
    "# # 26. split() - Split a simulated Address column into array (simulate Address)\n",
    "# df = df.withColumn(\"Address\", lit(\"123 Main St, Apt 4B, New York, NY\"))\n",
    "# df = df.withColumn(\"AddressParts\", split(col(\"Address\"), \", \"))\n",
    "\n",
    "# # 27. substring() - Extract first 3 characters from First Name\n",
    "# df = df.withColumn(\"NamePrefix\", substring(col(\"First Name\"), 1, 3))\n",
    "\n",
    "# # 28. sum() - Total sum of Salary (as aggregation example)\n",
    "# total_salary = df.agg(sum(\"Salary\").alias(\"TotalSalary\")).collect()[0][\"TotalSalary\"]\n",
    "\n",
    "# # 29. unix_timestamp() - Already used above as SimulatedUnix\n",
    "\n",
    "# # 30. upper() - Already used above as UpperName\n",
    "\n",
    "# # Show the final DataFrame with new columns (selecting a subset for clarity)\n",
    "# selected_columns = [\n",
    "#     \"First Name\", \"Gender\", \"Start Date\", \"Last Login Time\", \"Salary\", \"Bonus %\",\n",
    "#     \"Senior Management\", \"Team\", \"Bonus Amount\", \"Total Compensation\", \"Probation End Date\",\n",
    "#     \"UpperName\", \"LowerName\", \"FormattedSalary\", \"CumeDist\", \"CurrentDate\", \"CurrentTimestamp\",\n",
    "#     \"DenseRank_Salary\", \"DaysSinceStart\", \"StartDayOfMonth\", \"StartDayOfWeek\", \"StartDayOfYear\",\n",
    "#     \"DecodedData\", \"ExpSalary\", \"HireYear\", \"FloorSalary\", \"FormattedDate\", \"LoginHour\",\n",
    "#     \"PhoneIsNull\", \"PrevSalary\", \"NextSalary\", \"NameLength\", \"PaddedName\", \"StartMonth\",\n",
    "#     \"PhoneNumber\", \"SalaryMod10\", \"RandomValue\", \"RoundedSalary\", \"LoginSecond\", \"NamePrefix\"\n",
    "# ]\n",
    "# df.select(selected_columns).show(truncate=False)\n",
    "\n",
    "# # Show aggregation examples for max, min, and sum\n",
    "# print(\"Max Salary:\", max_salary)\n",
    "# print(\"Min Salary:\", min_salary)\n",
    "# print(\"Total Salary:\", total_salary)\n",
    "\n",
    "# # Show exploded skills example (from explode)\n",
    "# df_exploded = df.select(\"First Name\", explode(col(\"Skills\")).alias(\"Skill\"))\n",
    "# df_exploded.show()\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# # # Aggregated DataFrame by Team\n",
    "# # df_aggregated = df.groupBy(\"Team\").agg(\n",
    "# #     avg(\"Salary\").alias(\"Avg_Salary\"),\n",
    "# #     collect_list(\"First Name\").alias(\"Team_Members\"),\n",
    "# #     collect_set(\"First Name\").alias(\"Unique_Team_Members\")\n",
    "# # )\n",
    "\n",
    "# # # Join aggregated results back to the original DataFrame\n",
    "# # df_final = df.join(df_aggregated, on=\"Team\", how=\"left\")\n",
    "# # df_final.select(\"Team\", \"Avg_Salary\", \"Team_Members\", \"Unique_Team_Members\").show(truncate=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 7. Performance Optimization & Caching\n",
    "\n",
    "- Implement unit tests to verify data quality (check for null values, duplicate rows, and correct transformations).\n",
    "- Use assertions to ensure the integrity of key columns (e.g., “First Name” should have no nulls)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 8. Unit Testing (Data Quality Checks)\n",
    "\n",
    "You can run these tests in separate cells or in a dedicated testing notebook.\n",
    "\n",
    "- Save the final processed DataFrame to disk in Parquet (or CSV) format.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All data quality tests passed!\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "print(\"All data quality tests passed!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 9. Save Processed Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_final.repartition(2).write.format(\"parquet\").mode(\"overwrite\").save(\"/mount_folder/alpha/NYC_Taxi_Data_Pipeline_git/Practice/pyspark_functions/Employeee/employees_hr_processed.parquet\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Conclusion\n",
    "This notebook demonstrates a complete PySpark project that:\n",
    "\n",
    "- Ingests data,\n",
    "- Performs data cleaning and transformation,\n",
    "- Uses a comprehensive set of PySpark SQL functions (including statistical, date, window, and string functions),\n",
    "- Aggregates and analyzes the data,\n",
    "- Implements performance optimizations and unit tests,\n",
    "- And finally, saves the processed output.\n",
    "\n",
    "Feel free to adjust the sample data and configurations to match your real dataset from Kaggle or another open source."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
